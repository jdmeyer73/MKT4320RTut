[["index.html", "R Tutorial for MKT 4320 About this tutorial", " R Tutorial for MKT 4320 Jeffrey Meyer 2022-03-23 About this tutorial This tutorial is a quick and basic R tutorial for MKT 4320. It is not designed to be comprehensive, but will provide you with the basics of R, RStudio, and how to complete the types of analyses covered in the course. At this time, the tutorial is not complete, but will be updated as we move through the topics in the course. Much of the content in this tutorial is adapted from a variety of sources. Where applicable, the source will be noted. "],["r-basics.html", "Chapter 1 R Basics 1.1 Introduction 1.2 R Language 1.3 Packages 1.4 Getting Help 1.5 Basic Object Types (and Other Important Stuff) 1.6 Data Frames 1.7 Data Transformations 1.8 Loading and Saving Data 1.9 User Defined Functions 1.10 Package dplyr 1.11 Package lubridate 1.12 RMarkdown", " Chapter 1 R Basics Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html 1.1 Introduction 1.1.1 What is R? R is a programming language R is not a statistics program Much different than SPSS, MiniTab, Stata, etc. Does not use a point-and-click interface 1.1.2 Why R? Emerging techniques usually available in R quickly Default educational platform for statistics programs (and spreading to other disciplines) Large and diverse set of analytic tools Powerful plotting options Large community of helpful users R skills are in high demand And R is free 1.1.3 Why NOT R? Steep learning curve Requires programming 1.1.4 What is RStudio? RStudio IDE is an integrated development environment that makes working with R more user friendly Not required to use R, but provides a better interface and environment of writing code, editing code, and creating documents. Complete separate software And RStudio is free 1.1.5 Getting Started 1.1.5.1 Option 1: Install on your Machine Install R base from the Comprehensive R Archive Network (CRAN) https://cran.r-project.org/ Install RStudio https://www.rstudio.com/products/rstudio/download/#download 1.1.5.2 Option 2: Use BGSUs Virtual Computing Lab pilot See course website for link 1.1.5.3 Option 3: Use a computer in an on-campus computer lab All computer in on-campus computer labs should have R and RStudio installed 1.1.6 Navigating RStudio As you can see in Figure 1.1, RStudio is typically divided in 4 quadrants. Figure 1.1: RStudio Layout 1.1.6.1 Quadrant 1 The upper-left corner is the source pane. This pane is where the majority of your programming or coding will take place. When working in this pane, the code does not run unless you tell it to run. In addition, the code can be saved to prevent work from being lost. 1.1.6.2 Quadrant 2 The lower-left corner is the console pane. While it contains several tabs, generally the Console tab is the only one used. The console is where R actually evaluates the code. If you run code from the a source pane window, the code will automatically be put into the console pane and evaluated. You can also type code directly into the console pane and receive immediate results. Depending on the code, the results will appear directly in the console or in a tab Quadrant 4. The &gt; symbol shows that the console is ready for input. 1.1.6.3 Quadrant 3 The upper-right corner also contains several tabs, but the Environment and History tabs are the ones used most often. The Environment tab lists all data objects that have been defined in the current session. The History tab is an archive of all commands run in the current session. 1.1.6.4 Quadrant 4 The bottom-right corner also contains several tabs, all of which are used at different times. The Files tab lists all files in the current directory. The Plots tab shows visualizations created during the current session. The Packages tab shows all packages installed as well as which packages are currently loaded. The Help tab contains the help menu. The Viewer tab contains output from code if the code directs it to be displayed there. 1.2 R Language 1.2.1 Basics of R Commands R is case sensitive When using the console, use the keyboard  and  arrow keys to easily cycle through previous commands typed. When using the text editor (i.e., a script file) in the source pane, use the Ctrl+Enter keyboard shortcut to submit a line of code directly to the console. The entire line does not need to be highlighted; the cursor needs to be anywhere on the line to be submitted. When using the text editor/script file, the # symbol signifies a comment Everything after is ignored It can be on the same line: x &lt;- 100 # Assign 100 to x It can be on separate lines: # Assign 100 to x x &lt;- 100 1.2.2 Operators Mathematical and logical operators are used frequently. Table 1.1: R Operators Description Operator Mathematical addition \\(+\\) subtraction \\(-\\) multiplication \\(*\\) division \\(/\\) exponentiation ^ or \\(**\\) Logical less than &lt; less than or equal to &lt;= greater than &gt; greater than or equal to &gt;= exactly equal to == not equal to != Not x !x x OR y x|y x AND y x&amp;y test if X is TRUE isTRUE(x) 1.3 Packages Packages are collections of functions that have been written to expand the functionality of R. Packages are not automatically included, and if a command is given that is not part of a package installed and loaded, R will give an error message. Therefore, the package must first be installed, and then loaded. 1.3.1 Installing Packages Installing packages is performed with the install.packages(\"\")function, where the name of the package is inside the quotation marks Two packages that used quite often in this course are ggplot2 and dplyr install.packages(&quot;ggplot2&quot;) install.packages(&quot;dplyr&quot;) NOTE 1: Packages only need to be installed on your machine once NOTE 2: If using BGSUs Virtual Computing Lab, new packages cannot be installed, but the version in the virtual environment has most of the most common packages pre-installed 1.3.2 Loading Packages Once a package is installed, it has to be loaded with the library() function, where the name of the package is in the parentheses, in order to be used library(ggplot2) library(dplyr) NOTE 1: It is not uncommon to see a variety of messages when loading a package NOTE 2: Packages need to be loaded every time you start a new R session 1.4 Getting Help R has built in help to assist with understanding different functions To access the help, type ? FUNCTION in the console, and the help page for that function will show up in the lower-right pane under the help tab ? mean 1.5 Basic Object Types (and Other Important Stuff) Objects in R include variables, data sets, and functions The assignment operator &lt;- assigns a value to a named object x &lt;- 100 # Assign value 100 to object &#39;x&#39; x # Display object &#39;x&#39; [1] 100 NOTE: In RStudio console or script file, Alt+- will automatically paste the assignment operator As stated before, object names are case sensitive x &lt;- 100 # Assign value 100 to object &#39;x&#39; X # Display object &#39;X&#39; Error in eval(expr, envir, enclos): object &#39;X&#39; not found The print command can also be used to print objects print(x) [1] 100 1.5.1 Vectors Vectors can be created many ways and take many data types One method is to using the c() function, which concatenates individual items x.Num &lt;- c(1, 3.14, 5.49, 10, 20) x.Num [1] 1.00 3.14 5.49 10.00 20.00 x.Log &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) x.Log [1] TRUE FALSE TRUE TRUE FALSE x.Char &lt;- c(&quot;fr&quot;, &quot;fr&quot;, &quot;jr&quot;, &quot;so&quot;, &quot;sr&quot;) x.Char [1] &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; &quot;sr&quot; The class of the vector can be checked with the class() function # Concatenate class functions and print together print(c(class(x.Num), class(x.Log), class(x.Char))) [1] &quot;numeric&quot; &quot;logical&quot; &quot;character&quot; Vectors can only hold a single class/type of value When multiple classes are included, the values are coerced to the most general type x.Mix &lt;- c(1, FALSE, 3.5, &quot;Hello!&quot;) x.Mix [1] &quot;1&quot; &quot;FALSE&quot; &quot;3.5&quot; &quot;Hello!&quot; class(x.Mix) [1] &quot;character&quot; The c() function can be used to add to existing vectors, or combine vectors Type coercion will be applied as needed x2 &lt;- c(x.Num, 25, 50) x.Num [1] 1.00 3.14 5.49 10.00 20.00 x2 [1] 1.00 3.14 5.49 10.00 20.00 25.00 50.00 x3 &lt;- c(x2, &quot;Hello&quot;) x3 [1] &quot;1&quot; &quot;3.14&quot; &quot;5.49&quot; &quot;10&quot; &quot;20&quot; &quot;25&quot; &quot;50&quot; &quot;Hello&quot; Math can be applied directly to vectors x.Num [1] 1.00 3.14 5.49 10.00 20.00 x.Num + 100 [1] 101.00 103.14 105.49 110.00 120.00 x.Num * pi [1] 3.141593 9.864601 17.247344 31.415927 62.831853 The length() function provides the length of a vector length(x.Num) [1] 5 length(x.Char) [1] 5 The str() function provide the structure of an object Class, Length, and Value str(x.Num) num [1:5] 1 3.14 5.49 10 20 str(x.Char) chr [1:5] &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; &quot;sr&quot; 1.5.2 Indexing Indexing is used to obtain particular elements of a data structure Vectors are indexed with square brackets, [] # Obtain the third element of the &#39;x.Num&#39; vector x.Num[3] [1] 5.49 Items can be excluded with negative indexing # Obtain all elements except the third element of the &#39;x.Num&#39; vector x.Num[-3] [1] 1.00 3.14 10.00 20.00 Indexing also works with logical operators # Obtain all elements in &#39;x.Num&#39; greater than 5 x.Num[x.Num &gt; 5] [1] 5.49 10.00 20.00 1.5.3 Sequencing Vectors can also be created using sequencing Integer sequencing is done with #:# coding x.Seq &lt;- 1:10 x.Seq [1] 1 2 3 4 5 6 7 8 9 10 Sequences can also be used in indexing # Obtain the second through fourth element of the &#39;x.Num&#39; vector x.Num[2:4] [1] 3.14 5.49 10.00 Complex sequencing can be done using the seq() function x.Seq2 &lt;- seq(from=0, to=100, by=20) x.Seq2 [1] 0 20 40 60 80 100 Note: the from=, to= and by= can be excluded x.Seq3 &lt;- seq(0,100,20) x.Seq3 [1] 0 20 40 60 80 100 1.5.4 Missing (and Other Interesting) Values In R, missing values are assigned a special constant, NA NA is not a character value, but a type of its own Any math performed on a value of NA becomes NA x.Scores &lt;- c(85, 93, NA, NA) mean(x.Scores) [1] NA Many commands contain a option, na.rm=TRUE, to ignore NA data when performing the function mean(x.Scores, na.rm=TRUE) [1] 89 NA values can also be removed before performing the function using the na.omit() function mean(na.omit(x.Scores)) [1] 89 R also has special types for infinity, Inf, and undefined numbers (i.e., not a number), NaN To see this in action, take the natural log, log(), of certain numbers log(-1) Warning in log(-1): NaNs produced [1] NaN log(0) [1] -Inf Notice that R provides a warning when the NaN is found 1.5.5 Factors Character data can be converted into nominal factors using the as.factors() function Each unique character value will be a level of the factor Behind the scenes, R stores the values as integers, with a separate list of labels When the data type is set as a factor, R knows how to handle it appropriately in the model The levels can be accessed with the levels() function x.Class &lt;- as.factor(x.Char) str(x.Class) Factor w/ 4 levels &quot;fr&quot;,&quot;jr&quot;,&quot;so&quot;,..: 1 1 2 3 4 levels(x.Class) [1] &quot;fr&quot; &quot;jr&quot; &quot;so&quot; &quot;sr&quot; 1.6 Data Frames Data frames are an object type that deserve special attention 1.6.1 Overview Most common way to handle data sets in R Provide data to statistical functions Think of it like a spreadsheet, or a rectangular object where: Columns are varying data types (i.e., variables) Rows are values in each column (i.e, observations) 1.6.2 Creating a data frame Can construct a data frame with the data.frame() function Takes as input a set of vectors of the same length x.df &lt;- data.frame(x.Num, x.Log, x.Char) x.df x.Num x.Log x.Char 1 1.00 TRUE fr 2 3.14 FALSE fr 3 5.49 TRUE jr 4 10.00 TRUE so 5 20.00 FALSE sr Note that the column names are inherited from the contributing vector Elements can be indexed using [ROW, COLUMN] notation # Obtain the first element of the third row of data frame &#39;x.df&#39; x.df[3,1] [1] 5.49 # Obtain the third element of the second row of data frame &#39;x.df&#39; x.df[2,3] [1] &quot;fr&quot; 1.6.2.1 Using vectors in-line The vectors can be created in line (i.e., not already created) If the vectors are of different length, the shorter vector will be repeated IF the longest vector is divisible by the vector If a single value is provided instead of a vector, it is repeated for all rows x2.df &lt;- data.frame(var1=seq(10,100,10), var2=c(&quot;Yes&quot;,&quot;No&quot;), var3=1:5, var4=100) x2.df var1 var2 var3 var4 1 10 Yes 1 100 2 20 No 2 100 3 30 Yes 3 100 4 40 No 4 100 5 50 Yes 5 100 6 60 No 1 100 7 70 Yes 2 100 8 80 No 3 100 9 90 Yes 4 100 10 100 No 5 100 1.6.3 Viewing a Data Frame There are a few ways to view a data frame Type the data frame name in the console x.df x.Num x.Log x.Char 1 1.00 TRUE fr 2 3.14 FALSE fr 3 5.49 TRUE jr 4 10.00 TRUE so 5 20.00 FALSE sr With data frames that have many variables, this is cumbersome With data frames with many rows, a max.print setting will kick in and not all rows will be shown To view only a few rows of data, the head(DF, n) function can be used, where DF is the name of the data frame, and n (optional) is the number of rows to view, with 10 as the default NOTE: The Salaries data frame from the car package is being used as an example head(Salaries,5) rank discipline yrs.since.phd yrs.service sex salary 1 Prof B 19 18 Male 139750 2 Prof B 20 16 Male 173200 3 AsstProf B 4 3 Male 79750 4 Prof B 45 39 Male 115000 5 Prof B 40 41 Male 141500 Use the function View() or click on the data frame name in the environment tab to see the data farme in the Source pane See Figure 1.2 Figure 1.2: Viewing a Data Frame in the Source Window Indexing and Sequencing Indices can be left blank, which selects all of that dimension x.df[2, ] # Obtain all of row 2 x.Num x.Log x.Char 2 3.14 FALSE fr x.df[ ,3] # Obtain all of column 3 [1] &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; &quot;sr&quot; As with vectors, indexing can be done with sequencing and negative indices to omit rows x.df[2:3, ] # Obtain rows 2 and 3 x.Num x.Log x.Char 2 3.14 FALSE fr 3 5.49 TRUE jr x.df[ ,-2] # Exclude column 2 x.Num x.Char 1 1.00 fr 2 3.14 fr 3 5.49 jr 4 10.00 so 5 20.00 sr Indexing a data frame returns an object, but the object type depends on the indexing Choosing \\(n\\) rows and and a single column yields a vector of length \\(n\\) Choosing multiple columns returns a new data frame Use the str() function to verify the new objects structure str(x.df[1, 1]) # 1 row and 1 column = vector of length 1 num 1 str(x.df[1:3, 2]) # 3 rows and 1 column = vector of length 3 logi [1:3] TRUE FALSE TRUE str(x.df[1, 1:2]) # 1 row and 2 columns = 1 x 2 data frame &#39;data.frame&#39;: 1 obs. of 2 variables: $ x.Num: num 1 $ x.Log: logi TRUE str(x.df[1:4, c(1, 3)]) #4 rows and 2 columns = 4 x 2 data frame &#39;data.frame&#39;: 4 obs. of 2 variables: $ x.Num : num 1 3.14 5.49 10 $ x.Char: chr &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; Data frames can also be indexed using column names after the $ character x.df$x.Char [1] &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; &quot;sr&quot; 1.7 Data Transformations Analysts and researchers often need to create new variables from existing ones or transform existing variables Data transformations can usually be accomplished in more than one way Using base R will be shown first After the dplyr package is introduced, a second way will be shown For the transformations shown below, either a new variable will be added to the data frame, or an existing variable will be changed However, it is not always required that the data frame be changed Sometimes, a transformation will be done on the fly and used only in an analysis, but the data frame remains the same 1.7.1 Creating New Variables Adding a new variable is done using code such as df$new &lt;- SOME FUNCTION, where: df is the name of the data frame new is the name for the new variable SOME FUNCTION is the data transformation to take place # Create new variable &#39;NumSq&#39; that is the square of current variable x.Num x.df$NumSq &lt;- x.df$x.Num^2 x.df x.Num x.Log x.Char NumSq 1 1.00 TRUE fr 1.0000 2 3.14 FALSE fr 9.8596 3 5.49 TRUE jr 30.1401 4 10.00 TRUE so 100.0000 5 20.00 FALSE sr 400.0000 1.7.2 Recoding (Create New) In base R, recoding is usually a multi-step process using indexing # Recode &#39;x.Num&#39; into three factors x.df$Grp &lt;- &quot;low&quot; # Create new variable and set all rows to one level x.df$Grp[x.df$x.Num&gt;=3 &amp; x.df$x.Num&lt;10] &lt;- &quot;med&quot; # Create medium level x.df$Grp[x.df$x.Num&gt;10] &lt;- &quot;high&quot; # Create high level x.df$Grp &lt;- as.factor(x.df$Grp) # Set new variable as factor str(x.df) # See structure of data frame with new variable &#39;data.frame&#39;: 5 obs. of 5 variables: $ x.Num : num 1 3.14 5.49 10 20 $ x.Log : logi TRUE FALSE TRUE TRUE FALSE $ x.Char: chr &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; ... $ NumSq : num 1 9.86 30.14 100 400 $ Grp : Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;med&quot;: 2 3 3 2 1 Notice the &amp; in the code above. It stands for and, which tells R that both conditions must be true 1.7.3 Recoding (Change Existing) Recoding to change an existing variable is done in a similar manner NOTE: Sometimes we have to first change the variable type, such as when the existing variable is a factor # Recode &#39;Grp&#39; into only two factors # Change type to &#39;character&#39; x.df$Grp &lt;- as.character(x.df$Grp) # Recode &#39;low&#39; and &#39;med&#39; to &#39;very low&#39; x.df$Grp[x.df$Grp==&quot;low&quot; | x.df$Grp==&quot;med&quot;] &lt;- &quot;very low&quot; # Change back to factor x.df$Grp &lt;- as.factor(x.df$Grp) str(x.df) &#39;data.frame&#39;: 5 obs. of 5 variables: $ x.Num : num 1 3.14 5.49 10 20 $ x.Log : logi TRUE FALSE TRUE TRUE FALSE $ x.Char: chr &quot;fr&quot; &quot;fr&quot; &quot;jr&quot; &quot;so&quot; ... $ NumSq : num 1 9.86 30.14 100 400 $ Grp : Factor w/ 2 levels &quot;high&quot;,&quot;very low&quot;: 2 2 2 2 1 Notice the | in the code above. It stands for or, which tells R that either condition can be true. 1.8 Loading and Saving Data In this class, the data will be provided to you as an .Rdata file, which are specific to R and the best way to store (and load) objects. If desired, .Rdata files can contain multiple objects. For example, an .Rdata file could contain two different data frames. 1.8.1 Setting Working Directory To know where your files are saved to, and to make it easier to load files, it is probably best to have a working directory where existing files are located and where new files will be saved. Use the getwd() function to see your current working directory getwd() [1] &quot;C:/Users/jdmeyer/Docs/RFiles&quot; Set the working directory in one of three ways: Use the setwd() function: setwd(\"C:/Users/jdmeyer/Doc/RFiles/RTutorial\") NOTE: Forward slashes (as shown above) are used in place of backward slashes for directory paths Use the menus in RStudio SessionSet Working DirectoryChoose Directory See Figure 1.3 Figure 1.3: Set Working Direcory using Menus Use the Files tab in the lower-right corner in RStudio See Figure 1.4 Figure 1.4: Set Working Direcory using Files Tab 1.8.2 Loading .Rdata Use the load(\"FILE LOCATION/NAME\") function to load and .Rdata file Suppose the data set used in Topic 1 (airlinesat.rdata) is in a subdirectory of the working directory called Data load(&quot;Data/airlinesat.rdata&quot;) See 1.5 for a visual example Figure 1.5: Loading Data 1.8.3 Saving .Rdata Use the save(OBJECT, file=\"SAVENAME\") function, where: OBJECT is one or more objects in the current environment separated by commas SAVENAME is the name of the file the objects will be saved to. save(x.df, file=&quot;xdf.rdata&quot;) 1.9 User Defined Functions Often, a series of commands is repeated over and over, or there is not an easy, built-in function in R to provide a needed result. In these case, R makes it relatively easy to create your own functions 1.9.1 Structure A function is constructed in the in the following manner: function.name &lt;- function(arglist) {body} where function.name is the name of the user defined function (arglist) contains the names of any inputs to the function, separated by commas {body} contains the code that operates on the inputs The function must be executed prior to using it in the current session; alternatively, the function can be saved as a .R script file and sourced using the source() function in R 1.9.2 Example Suppose a function is need to take the square root of a natural log transformation The new function will be named sqlog and will have one argument # Define the new function sqlog &lt;- function(x) { # Function has one argument, &#39;x&#39; sqrt(log(x)) # What is done with the argument, &#39;x&#39; } Now that the function is created and ran, it can be used. In this case, the output will be printed to the console. The function could have been written in a way that the result is stored as an object. sqlog(100) # Using the function on a single number [1] 2.145966 myvals &lt;- c(5, 10, 20, 40, 80, 160) # Creating a vector of values sqlog(myvals) # Using the function on a vector [1] 1.268636 1.517427 1.730818 1.920646 2.093329 2.252815 1.10 Package dplyr The dplyr package (pronounced DEE ply er) is a package that makes data manipulation much easier and more intuitive (for most). dplyr is built around the five main verbs shown below that make up a majority of data manipulation. However, there are other functions that dplyr uses to also help with data manipulation. select is used to subset columns filter is used to subset rows arrange is used to sort rows mutate is used to add new columns based on calculations (usually with other columns) summarise is use to perform summary calculations (e.g., mean, max, etc.) on data set In addition, dplyr uses the pipe, %&gt;%, to string together a series of functions. Think of functions strung together as upstream and downstream functions. The function to the left of %&gt;% is the upstream function, while the function to the right is the downstream function. By default, the downstream function assumes the value coming from the upstream function is the first argument in its function Therefore, the first argument can be omitted If the downstream function needs to use the value from from the upstream function assigned to a different argument, a . is simply put in the position of that argument 1.10.1 dplyr Examples First, be sure the dplyr package is loaded: library(dplyr) Well be using the airlinesat dataset from the airlinesat.rdata file for these examples. 1.10.1.1 select() function Usage: select(.data, ...), where ... is one or more unquoted expressions separated by commas airlinesat has 70 variables, but 46 of them are a series of expectation (e1 to e23) and satisfaction (s1 to s23)scales. First, we want to create a new data frame with those variables excluded. For simplicity, the num_range(\"prefix\", start:finish) selection can be used, and the ! takes the complement (i.e., all but these) airlinesat.small &lt;- # Create new data frame airlinesat %&gt;% # Use airlinesat as the starting data frame select(!num_range(&quot;e&quot;, 1:23)) %&gt;% # Select all but e1 to e23 select(!num_range(&quot;s&quot;, 1:23)) # Select all but s1 to s23 # NOTE: the two selects could have been concantendated as: # select(!c(num_range(&quot;e&quot;, 1:23), num_range(&quot;s&quot;, 1:23))) str(airlinesat.small) &#39;data.frame&#39;: 1065 obs. of 24 variables: $ age : num 30 55 56 43 44 40 39 41 33 51 ... $ country : Factor w/ 5 levels &quot;at&quot;,&quot;ch&quot;,&quot;de&quot;,..: 2 2 2 4 2 2 2 2 2 3 ... $ flight_class : Factor w/ 3 levels &quot;Business&quot;,&quot;Economy&quot;,..: 2 1 2 2 1 3 2 1 2 1 ... $ flight_latest : Factor w/ 6 levels &quot;within the last 12 months&quot;,..: 4 3 5 3 6 5 6 3 3 4 ... $ flight_purpose: Factor w/ 2 levels &quot;Business&quot;,&quot;Leisure&quot;: 2 1 1 2 1 2 1 1 2 1 ... $ flight_type : Factor w/ 2 levels &quot;Domestic&quot;,&quot;International&quot;: 1 2 1 1 2 2 1 2 1 2 ... $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 1 1 2 2 2 2 2 ... $ language : Factor w/ 3 levels &quot;English&quot;,&quot;French&quot;,..: 2 1 1 2 1 3 2 2 2 3 ... $ nflights : num 2 6 8 7 25 16 35 9 3 4 ... $ status : Factor w/ 3 levels &quot;Blue&quot;,&quot;Gold&quot;,..: 1 2 1 1 2 2 1 2 1 2 ... $ nps : num 6 10 8 8 6 7 8 7 8 8 ... $ sat1 : num 5 6 4 6 4 7 6 5 4 4 ... $ sat2 : num 2 6 2 6 3 2 3 3 4 4 ... $ sat3 : num 4 6 2 4 2 7 5 4 4 3 ... $ loy1 : num 3 6 3 6 3 2 2 4 4 3 ... $ loy2 : num 3 6 4 4 3 3 7 4 4 4 ... $ loy3 : num 3 6 7 5 2 2 7 4 2 3 ... $ loy4 : num 7 5 2 4 2 3 7 5 1 3 ... $ loy5 : num 3 6 4 5 2 2 6 5 2 4 ... $ com1 : num 1 6 7 6 3 7 7 4 3 1 ... $ com2 : num 1 6 7 6 3 6 7 4 3 7 ... $ com3 : num 7 6 7 6 2 3 7 5 3 7 ... $ overall_sat : num 2 6 2 4 2 4 4 4 4 3 ... $ reputation : num 3 6 4 6 5 3 3 4 2 4 ... Second, we want to create a new data frame with only demographic variables airlinesat.d &lt;- # Create new data frame airlinesat %&gt;% # Use airlinesat as the starting data frame select(age, country, gender) # Select only demographic variables str(airlinesat.d) &#39;data.frame&#39;: 1065 obs. of 3 variables: $ age : num 30 55 56 43 44 40 39 41 33 51 ... $ country: Factor w/ 5 levels &quot;at&quot;,&quot;ch&quot;,&quot;de&quot;,..: 2 2 2 4 2 2 2 2 2 3 ... $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 1 1 2 2 2 2 2 ... 1.10.1.2 filter() function Usage: filter(.data, ...), where ... is an expression that returns a logical value First, we want to select only those rows where the person is from the United States (i.e., country == \"us\") using the newly created airlinesat.d data frame airlinesat.dus &lt;- airlinesat.d %&gt;% filter(country == &quot;us&quot;) head(airlinesat.dus) age country gender 1 52 us male 2 68 us male 3 64 us female 4 47 us male 5 57 us male 6 43 us male Second, we want to select only those rows where the person is older than the mean age using the newly created airlinesat.d data frame airlinesat.dold &lt;- airlinesat.d %&gt;% filter(age &gt; mean(age, na.rm=TRUE)) head(airlinesat.dold) age country gender 1 55 ch male 2 56 ch female 3 51 de male 4 58 de female 5 53 de male 6 53 de male 1.10.1.3 arrange() function Usage: arrange(.data, ..., .by_group = FALSE) where ... are variable(s) or functions of variables Use desc(...) to sort in a descending order .by_group = TRUE will sort first by a grouping variable, if one exists; default is FALSE First, we want to see the first 10 rows in ascending order of age using the airlinesat.d data frame # NOTE: A new data frame does not need to be created. # The result of the data manipulation can be sent directly # to another function, like &#39;head()&#39; airlinesat.d %&gt;% arrange(age) %&gt;% head(10) age country gender 1 19 de male 2 19 de male 3 21 de male 4 21 de male 5 22 de female 6 22 de male 7 22 de male 8 22 de female 9 22 de male 10 23 de male Second, we want to see the first 10 rows of three variables (age, country, nflights) from the original airlinesat data frame, sorted first ascending by age and second descending by nflights airlinesat %&gt;% select(age, country, nflights) %&gt;% arrange(age, desc(nflights)) %&gt;% head(10) age country nflights 1 19 de 15 2 19 de 3 3 21 de 11 4 21 de 2 5 22 de 20 6 22 de 8 7 22 de 3 8 22 de 3 9 22 de 2 10 23 de 15 1.10.1.4 mutate() function Usage: mutate(.data, ...) where ... are name-value pairs. The name gives the name of the new column/variable. The value is some function or formula. First, we want to create a standard normal variable for age using the airlinesat.d data frame airlinesat.d %&gt;% mutate(age_snrm=(age-mean(age, na.rm=TRUE))/sd(age,na.rm=TRUE)) %&gt;% head(10) age country gender age_snrm 1 30 ch male -1.66357002 2 55 ch male 0.37315007 3 56 ch female 0.45461887 4 43 fr female -0.60447557 5 44 ch female -0.52300677 6 40 ch male -0.84888198 7 39 ch male -0.93035079 8 41 ch male -0.76741318 9 33 ch male -1.41916361 10 51 de male 0.04727486 Second, we want to use mutate with recode to create a new variable, continent based on variable country recode(.x, ...) where .x is the variable to modify and ... are the things to recode in old = \"new\" format separated by commas Use recode_factor(.x, ...) to recode factor variables airlinesat.d %&gt;% mutate(continent=recode_factor(country, at=&quot;Europe&quot;, ch=&quot;Europe&quot;, de=&quot;Europe&quot;, fr=&quot;Europe&quot;, us=&quot;North America&quot;)) %&gt;% head(10) age country gender continent 1 30 ch male Europe 2 55 ch male Europe 3 56 ch female Europe 4 43 fr female Europe 5 44 ch female Europe 6 40 ch male Europe 7 39 ch male Europe 8 41 ch male Europe 9 33 ch male Europe 10 51 de male Europe 1.10.1.5 summarise() function Usage: summarise(.data, ...) where ... are name-value pairs of summary functions (e.g., mean(), min(), sum(), n()) More than one summary function can be included First, we want to find the mean, standard deviation, and number of valid observations for the nflights variable airlinesat %&gt;% summarise(mean_flights=mean(nflights, na.rm=TRUE), sd_flights=mean(nflights, na.rm=TRUE), valid_n=sum(!is.na(nflights))) mean_flights sd_flights valid_n 1 13.41878 13.41878 1065 Second, we want to find the same information as above, but by continent group Use the group_by() function before summarise() # NOTE: The output will be of type &#39;tibble&#39; instead of &#39;data.frame&#39; # Tibbles are like data frames, but occasionally behave differently airlinesat %&gt;% mutate(continent=recode_factor(country, at=&quot;Europe&quot;, ch=&quot;Europe&quot;, de=&quot;Europe&quot;, fr=&quot;Europe&quot;, us=&quot;North America&quot;)) %&gt;% group_by(continent) %&gt;% summarise(mean_flights=mean(nflights, na.rm=TRUE), sd_flights=mean(nflights, na.rm=TRUE), valid_n=sum(!is.na(nflights))) # A tibble: 2 x 4 continent mean_flights sd_flights valid_n &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 Europe 13.7 13.7 870 2 North America 12.2 12.2 195 1.11 Package lubridate The lubridate package was created to make working with dates and times a little bit easier. While the package has a great deal of funcationality, this tutorial will only focus on a some of the most common elements. Probably the most useful capability of lubridate is its ability to quickly parse out parts of a text date string, no matter the form of that string. Fast parsing is done through a series of functions that are based on the order of the parts of date/time in the text string. Use ymd() if the order is year, month, day, such as 20101215 or 16/7/1 Use mdy() if the order is month, day, year, such as January 25, 2016 or 5/29/1993 Use dmy() if the order is day, month, year, such as 171210 or 5 January 1990 The results will be the in the format YYYY-MM-DD and have a Date class library(lubridate) ymd(c(&quot;20101215&quot;, &quot;16/7/1&quot;)) [1] &quot;2010-12-15&quot; &quot;2016-07-01&quot; mdy(c(&quot;January 25, 2016&quot;, &quot;5/29/1993&quot;)) [1] &quot;2016-01-25&quot; &quot;1993-05-29&quot; dmy(c(&quot;171210&quot;, &quot;5 January 1990&quot;)) [1] &quot;2010-12-17&quot; &quot;1990-01-05&quot; If time is also included in the text string, that can also be parsed out, such as ymd_hms() where hms stands for hours, minutes, seconds Additionally, lubridate can easily pull out the specific components that are have class Date year(x) returns the year number month(x) returns the number of the month month(x, label=TRUE) returns the month name (abbreviated) day(x) (or mday(x)) returns the number of the day of the month wday(x) returns the number of the day in week, where Sunday = 1 wday(x, label=TRUE) returns the day of the week name (abbreviated) yday(x) returns the number of the day of the year # Use lubridate to create a date class object for my birthday (not really) mybday &lt;- mdy(&quot;March 15, 1973&quot;) year(mybday) [1] 1973 month(mybday) [1] 3 month(mybday, label=TRUE) [1] Mar 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec day(mybday) [1] 15 wday(mybday) [1] 5 wday(mybday, label=TRUE) [1] Thu Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat yday(mybday) [1] 74 1.12 RMarkdown RMarkdown allows users to create documents that combine code and text. Ultimately, they are used for reproducible research. That is, the R Markdown files allow other people to see exactly what was done, and if the data is available to all, other people can reproduce the research. In this class, RMarkdown will be used for the lab assignments. You will write your code in the RMarkdown file, and answer the assignment questions with the code your have written. While RMarkdown has a great deal of functionality, we will focus on a select few of those features. This cheatsheet may be beneficial. 1.12.1 YAML Header The top of every RMarkdown is a section called the YAML header, which is enclosed at the top and bottom with ---. For this class, the only think in the YAML header that you will need to change is the author: section. Figure 1.6: Change Author in YAML Header 1.12.2 Markdown Syntax When writing text in RMarkdown, there isnt a point and click menu to change fonts size or to add italics or bold text or to insert an equation. Instead, it requires the use of markdown syntax. The most commonly used are provided below: Plain text Plain text *Italic* Italic **Bold** Bold ***Bold and Italic*** Bold and Italic # Header 1 Header 1 ## Header 2 Header 2 ### Header 3 Header 3 #### Header 4 Header 4 * Unordered list item * Item 2 * Item 2a (indent 4 spaces) * Item 2b Unordered list item Item 2 Item 2a (indent 4 spaces) Item 2b 1. Ordered list item 2. Item 2 1. Item 2a (indent 4 spaces) 2. Item 2b Ordered list item Item 2 Item 2a (indent 4 spaces) Item 2b `verbatim code` vertabim code equation: $y=\\alpha+\\betax$ equation: \\(y=\\alpha+\\beta x\\) (See this file for symbols) 1.12.3 Code Chunks One of the greatest things about RMarkdown files is the ability to include code, and run the code, within the RMarkdown file. Putting code in the file is done with code chunks. 1.12.3.1 Inserting a Code Chunk Code chunks can be inserted by clicking on the Insert Chunk button in the document toolbar (see Figure 1.7) or by manually typing in the code chunk (see Figure 1.8) Figure 1.7: Inserting a Code Chunk from the Toolbar Figure 1.8: Inserting a Code Chunk Manually 1.12.3.2 Using Code Chunks RMarkdown doesnt pay attention to anything else going on in your R/RStudio session Any objects, data, packages, or user defined scripts must be in a code chunk in RMarkdown However, once they are loaded in a chunk, they do not need to be loaded again in subsequent chunks To run a code chunk, click on the green triangle ( If you run a chunk that does have the necessary object, data, etc., you will get an error message (see Figure 1.9) Figure 1.9: Chunk Error If you need to run previous chunks to load data or packages, click on the gray triangle with the green bar underneath () first, then click on the green triangle (see Figure 1.10) Figure 1.10: Run Previous Chunks, then Run Chunk Once the chunk runs correctly, the result will be shown under the chunk To clear the result, click on the x in the upper right hand corner of the results 1.12.3.3 Knitting the Results At any time, you can Knit the RMarkdown file to an HTML document or a PDF document. While working through the document, HTML is usually quicker. When the document is completed, a PDF is more professional looking (and for this course, required to turn in the lab assignment). To Knit the file, click on the down arrow next to Knit in the document toolbar, and select how you would like to product the document (see Figure 1.11) By default, a document knitted to HTML will be viewable in the Viewer window in the lower right hand side of RStudio. For documents knitted to PDF, RStudio generally opens up a new window with the knitted PDF, from which the file can be saved to a local directory Figure 1.11: Knit a Document "],["examining-and-summarizing-data.html", "Chapter 2 Examining and Summarizing Data 2.1 Introduction 2.2 Visualizations 2.3 Tables and Statistics", " Chapter 2 Examining and Summarizing Data Sources for this chapter: ggplot2: https://ggplot2.tidyverse.org/ 2.1 Introduction Examining and summarizing data involves visualizations (e.g., graphs and charts) and tables. For visualization, the most popular package in R is th ggplot2 package. For this part of the tutorial, the airlinesat.rdata data file is used, which contains the airlinesat dataframe. You should load this data now:load(\"airlinesat.rdata\") 2.2 Visualizations 2.2.1 Package ggplot2 2.2.1.1 Introduction First, make sure the ggplot2 library is loaded: library(ggplot2) ggplot2 is based on the basic idea that for a graph, we need three things: Data: ggplot2 is designed to work with data frames Coordinate system: what are the x and y variables Geoms: how is the data being represented (e.g., points, lines, etc.) AND that the plot can be enhance by adding more layers, using + When ggplot is used in the console or from a script, the plot appears in the Viewer tab of the lower-right corner 2.2.1.2 Usage A plot starts with the ggplot() function, which requires two arguments: Source of the data, which can be piped in (i.e., %&gt;%) The mapping of the data components This argument is an aesthetic function, aes(), which maps the variable(s) to the coordinate system If the ggplot() function alone is used, the output is simply the coordinate system, but with nothing plotted Because a geom hasnt been requested ggplot(airlinesat, # Use data frame &#39;airlinesat&#39; aes(x=country, y=nflights)) # Map &#39;country&#39; on x and &#39;nflights&#39; on y Figure 2.1: Call to ggplot() without a geom Other parts of the plot are adding in layers, using + A good analogy is building a house: The call to ggplot() is the foundation, but the structure is built one layer at a time Example: Request a column chart for a discrete x and a continuous y ggplot(airlinesat, # Use data frame &#39;airlinesat&#39; aes(x=country, y=nflights)) + # Map &#39;country&#39; on x and &#39;nflights&#39; on y geom_col() # Ask for column chart as the geom Figure 2.2: ggplot() + geom_col() NOTE: Each geom has a default statistic to plot In this case, it is summing the nflights variable by country We can use dplyr and ggplot2 together to get a different value, such as the mean # Use airlinesat data airlinesat %&gt;% # Group data by &#39;country&#39; group_by(country) %&gt;% # Create summary statistic summarise(mean_nflights=mean(nflights, na.rm=TRUE)) %&gt;% # Pass this results to ggplot and start the plot ggplot(aes(x=country, y=mean_nflights)) + # Note dataset was `piped` # Request column geom geom_col() Figure 2.3: Using dplyr before ggplot to get mean values Using ggplot() can get much more advanced. As the tutorial progresses, many examples of additional layers to a ggplot() will be shown. 2.2.2 Bar and Column Charts In ggplot, bar charts, geom_bar(), are used for plotting a single discrete variable, while column charts, geom_col(), are used for plotting a discrete variable on the x axis and a continuous variable on the y axis. 2.2.2.1 Bar Charts The standard bar chart provides a count of observations of each category of discrete variable x airlinesat %&gt;% ggplot(aes(x=gender)) + geom_bar() Figure 2.4: Standard bar chart To get percentages of each category, we need to summarize the data and calculate the proportion for each category airlinesat %&gt;% group_by(gender) %&gt;% # Group data by gender summarise(n=n()) %&gt;% # Create variable with count of each gender mutate(prop=n/sum(n)) %&gt;% # Create variable with proportion by gender ggplot(aes(x=gender, # Variable for the x-axis y=prop)) + # Use &#39;prop&#39; instead of default counts for y-axis geom_bar(stat=&quot;identity&quot;) # Use the value of y as-is Figure 2.5: Bar chart with proportions To make the chart pretty, we change the color of each bar we can add layers for axis labels, use the scales package to have the y-axis show percent, add labels for the bars etc. airlinesat %&gt;% group_by(gender) %&gt;% # Group data by gender summarise(n=n()) %&gt;% # Create variable with count of each gender mutate(prop=n/sum(n)) %&gt;% # Create variable with proportion by gender ggplot(aes(x=gender, # Variable for the x-axis y=prop, # Use &#39;prop&#39; instead of default counts for y-axis fill=gender)) + # Use different color for each bar geom_bar(show.legend=FALSE, # Hide legend stat=&quot;identity&quot;, ) + # Use the value of y as-is scale_y_continuous(labels=scales::label_percent()) + # y-axis labels % labs(x=&quot;Gender&quot;, y=&quot;Percent&quot;) + # Label x- and y-axes geom_text(aes(label=sprintf(&quot;%.1f%%&quot;, prop*100)), # Format label number vjust=.95, # Vertically adjust the labels fontface=&quot;bold&quot;, # Bold typeface color=&quot;white&quot;) # Text color Figure 2.6: Prettier bar chart 2.2.2.1.1 Bar Chart Variations 2.2.2.1.1.1 Stacked Bar Chart Used to show one discrete variable by another discrete variable, such as data you would see in a cross-tab The x= variable specifies the axis, while the fill= variable stacks the bars by the other variable As with other bar charts, the default is to count observations, so some manipulation is needed to get 100% stacked bar charts airlinesat %&gt;% group_by(gender, flight_type) %&gt;% # Group data by two discrete variables summarise(n=n()) %&gt;% # Count observations for each combination mutate(prop=n/sum(n)) %&gt;% # Calculate prop WITHIN first grouping variable ggplot(aes(x=gender, y=prop, fill=flight_type)) + geom_bar(position=&quot;fill&quot;, # Stack the bars stat=&quot;identity&quot;) + # Use the value of y as-is scale_y_continuous(labels=scales::label_percent()) + # y-axis labels % labs(x=&quot;Gender&quot;, y=&quot;Percent&quot;, # Label x- and y-axes fill=&quot;Flight Type&quot;) + # Label legend geom_text(aes(label=sprintf(&quot;%.1f%%&quot;, prop*100)), # Format data label position=position_stack(vjust=.95), # Adjust the labels fontface=&quot;bold&quot;, # Bold typeface color=&quot;white&quot;) # Text color Figure 2.7: Stacked bar chart 2.2.2.1.1.2 Side-by-Side Bar Chart Also used to shows one discrete variable by another discrete variable Again, default is to count observations, so some manipulation required to get percentages Percentages can be within a group (like in 100% stacked, see Figure 2.8) or percent of overall total (see Figure 2.9) # NOTE: The code for this chart is nearly identical to the previous figure # ONLY the changes have been commented on below airlinesat %&gt;% group_by(gender, flight_type) %&gt;% summarise(n=n()) %&gt;% mutate(prop=n/sum(n)) %&gt;% ggplot(aes(x=gender, y=prop, fill=flight_type)) + # NOTE: Use position=&quot;dodge&quot; to make bars side-by-side geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + scale_y_continuous(labels=scales::label_percent()) + labs(x=&quot;Gender&quot;, y=&quot;Percent&quot;, fill=&quot;Flight Type&quot;) + # NOTE: Use position=position_dodge(width=1) to position labels # in center of each bar horizontally; use vjust=.95 to # position labels at the top of each bar geom_text(aes(label=sprintf(&quot;%.1f%%&quot;, prop*100)), position=position_dodge(width=1), vjust=.95, fontface=&quot;bold&quot;, color=&quot;white&quot;) Figure 2.8: Side-by-side bar chart (% within group) # NOTE: The code for this chart is nearly identical to the previous figure # ONLY the changes have been commented on below airlinesat %&gt;% group_by(gender, flight_type) %&gt;% # NOTE: Use .groups=&quot;drop&quot; to remove the grouping structure after # summarising the data summarise(n=n(), .groups=&quot;drop&quot;) %&gt;% mutate(prop=n/sum(n)) %&gt;% ggplot(aes(x=gender, y=prop, fill=flight_type)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) + scale_y_continuous(labels=scales::label_percent()) + labs(x=&quot;Gender&quot;, y=&quot;Percent&quot;, fill=&quot;Flight Type&quot;) + geom_text(aes(label=sprintf(&quot;%.1f%%&quot;, prop*100)), position=position_dodge(width=1), vjust=.95, fontface=&quot;bold&quot;, color=&quot;white&quot;) Figure 2.9: Side-by-side bar chart (% of total) 2.2.2.2 Column Charts The standard column chart provides a sum of continuous variable y of each category of disrete variable x airlinesat %&gt;% ggplot(aes(x=flight_type, y=nflights)) + geom_col() Figure 2.10: Standard column chart To get a different summary statistic, such as mean, we can summarize the data and calculate the summary statistic for each category (and make the graph prettier) airlinesat %&gt;% group_by(flight_type) %&gt;% summarise(mean=mean(nflights)) %&gt;% ggplot(aes(x=flight_type, y=mean, fill=flight_type)) + geom_col(show.legend=FALSE) + labs(x=&quot;Flight Type&quot;, y=&quot;Mean Number of Flights&quot;) + geom_text(aes(label=sprintf(&quot;%.2f&quot;, mean)), # Format label number vjust=.95, # Vertically adjust the labels fontface=&quot;bold&quot;, # Bold typeface color=&quot;white&quot;) # Text color Figure 2.11: Column chart showing means 2.2.2.2.1 Side-by-Side Column Chart A side by side column chart can be used to show two discrete variables on the x-axis airlinesat %&gt;% group_by(flight_type, flight_purpose) %&gt;% summarise(mean=mean(nflights), .groups=&quot;drop&quot;) %&gt;% ggplot(aes(x=flight_type, y=mean, fill=flight_purpose)) + geom_col(position=&quot;dodge&quot;) + labs(x=&quot;Flight Type&quot;, y=&quot;Mean Number of Flights&quot;, fill=&quot;Flight Purpose&quot;) + geom_text(aes(label=sprintf(&quot;%.2f&quot;, mean)), position=position_dodge(width=1), vjust=.95, fontface=&quot;bold&quot;, color=&quot;white&quot;) Figure 2.12: Side-by-side column chart 2.2.3 Histogram In ggplot, histograms are produced with the geom_histogram() geom, which produces a histogram of a single continuous variable. By default, the y-axis is a count of observations in each bin of the x variable A bin is a range of values of the continuous x variable By default, ggplot will produce a histogram with 30 bins, and a message is produced to that effect unless the bins are changed manually airlinesat %&gt;% ggplot(aes(x=age)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 2.13: Standard histogram 2.2.3.1 Changing Bins Histograms can look quite different based on the bins used. Bins can be changed in two ways: (1) number of bins; and (2) bin width Changing the number of bins is done with the bins= option For example: geom_histogram(bins=20) Changing the bin width is done with the binwidth= option For example; geom_histogram(binwidth=5) Use the interactive histograms (Figure 2.14 and Figure 2.15 to see how the histograms change Figure 2.14: Interactive histogram for number of bins Figure 2.15: Interactive histogram for bin width 2.2.3.2 Improving the Look You may find the default histogram a little blah or tough to read. Just as the look of bar and column charts could be changed, so can the look of histograms airlinesat %&gt;% ggplot(aes(x=age)) + geom_histogram(color=&quot;black&quot;, # Adds red border around each bar fill=&quot;tan&quot;) + # Makes each bar blue labs(x=&quot;Age&quot;, y=&quot;Frequency&quot;) Figure 2.16: Prettier histogram 2.2.3.3 Other Options Instead of the default count of observations, a density histogram can be created, where the sum of the area of the bars adds up to 1 Often, a normal curve is added look of bar and column charts could be changed, so can the look of histograms airlinesat %&gt;% ggplot(aes(x=age)) + geom_histogram(aes(y=..density..), # Request density instead of count color=&quot;black&quot;, # Adds red border around each bar fill=&quot;tan&quot;) + # Makes each bar blue stat_function(fun=function(x) # Adds normal curve ovarlay dnorm(x, mean=mean(airlinesat$age, na.rm=TRUE), # Mean of normal dist sd=sd(airlinesat$age, na.rm=TRUE))) + # StdDev of normal dist labs(x=&quot;Age&quot;, y=&quot;Density&quot;) Figure 2.17: Density histogram with normal curve 2.2.4 Box Plot Box Plots are drawn with the geom_boxplot() geom, which by default creates a box plot for a continuous y variable, but for each level of a discrete x variable. In addition, the standard box plot does not contain whiskers. To get a box plot for only the continuous y variable, use x=\"\" as the discrete x variable To add whiskers, include a stat_boxplot(geom=\"errorbar\") layer airlinesat %&gt;% ggplot(aes(x=&quot;&quot;, y=age)) + geom_boxplot() + stat_boxplot(geom=&quot;errorbar&quot;) + # Add whiskers to box plot labs(x=&quot;&quot;, # Remove x axis label y=&quot;Age&quot;) # Make y axis label nicer Figure 2.18: Single box plot with whiskers To make comparisons across a discrete x variable, replaces the x=\"\" from before with x=VARIABLE airlinesat %&gt;% ggplot(aes(x=flight_purpose, y=age)) + geom_boxplot() + stat_boxplot(geom=&quot;errorbar&quot;) + # Add whiskers to box plot labs(x=&quot;Flight Purpose&quot;, y=&quot;Age&quot;) Figure 2.19: Multiple box plot 2.2.5 Scatterplot Scatterplots are drawn with the geom_point() geom and are used to show the relationship between two continuous variables Notice the warning given due to missing values (these warnings will be suppressed in other scatterplots below) airlinesat %&gt;% ggplot(aes(x=age, y=s10)) + # s10 is satisfaction with condition of airplane geom_point() Warning: Removed 40 rows containing missing values (geom_point). Figure 2.20: Standard scatterplot 2.2.5.1 Trendline Scatterplots become more helpful when we add a trend line. The most common trend line is a simple regression line, although others can be used. Use geom_smooth(method=\"lm\", se=FALSE) to add a linear trend line airlinesat %&gt;% ggplot(aes(x=age, y=s10)) + # s10 is satisfaction with condition of airplane geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) + # Add trendline labs(x=&quot;Age&quot;, y=&quot;Satisfaction with Aircraft Condition&quot;) Figure 2.21: Scatterplot with trendline 2.2.5.2 Other Options The color, https://ggplot2.tidyverse.org/articles/ggplot2-specs.html#sec:shape-spec, and size of the points can be changed In addition, they can vary by levels of a discrete variable If a trend line is requested, separate trend lines will be provided for each level of the discrete variable airlinesat %&gt;% ggplot(aes(x=age, y=s10, color=flight_type)) + geom_point(shape=17) + geom_smooth(method=&quot;lm&quot;, se=FALSE) + # Add trendline labs(x=&quot;Age&quot;, y=&quot;Satisfaction with Aircraft Condition&quot;, color=&quot;Flight Type&quot;) Figure 2.22: Scatterplot with different colors for discrete variable 2.3 Tables and Statistics 2.3.1 Frequency Table 2.3.1.1 Base R The table(data$variable) function can produce a one-way frequency table Wrapping the call to table with proportions will create the table with proportions (i.e., percent in each category) table(airlinesat$language) # One way frequency table (counts) English French German 233 10 822 proportions(table(airlinesat$language)) # One way table of proportions English French German 0.218779343 0.009389671 0.771830986 Table 2.1: One-way frequency table using Base R 2.3.1.2 Package questionr The freq() command from the package questionr produces nice one-way frequency tables (i.e., a frequency table for a single discrete variable) library(questionr) freq(airlinesat$language, # Provide discrete variable cum=TRUE, # Add cumulative percent column total=TRUE) # Add total row at bottom n % val% %cum val%cum English 233 21.9 21.9 21.9 21.9 French 10 0.9 0.9 22.8 22.8 German 822 77.2 77.2 100.0 100.0 Total 1065 100.0 100.0 100.0 100.0 Table 2.2: One-way frequency table using questionr Use package htmlTable() to produce a nicer looking table for output in the viewer (vs. the console) library(htmlTable) freq(airlinesat$flight_type, cum=TRUE, total=TRUE) %&gt;% # Pass result to htmlTable htmlTable() n % val% %cum val%cum Domestic 558 52.4 52.4 52.4 52.4 International 507 47.6 47.6 100 100 Total 1065 100 100 100 100 Table 2.3: One-way frequency table using htmlTable 2.3.2 Crosstabs 2.3.2.1 Base R Base R does not do a great job of easily creating cross-tabs and testing for independent of the two variables Using base R, a multistep process is required Create the two-way frequency table using the table(rowvar, colvar) function and assign it to a separate object Display the two-way freq table by just using the table name Use the function proportions(tablename, margin) on the newly created object to get column, row, or total percentages proportions(tablename) gives total percentages proportions(tablename, 1) gives row percentages proportions(tablename, 2) gives column percentages Use the function chisq.test(tablename) on the newly created object to run the test of independence # Create two way table crosstab &lt;- table(airlinesat$flight_purpose, # row Variable airlinesat$gender) # Column variable crosstab # Display 2-way freq table female male Business 76 449 Leisure 204 336 proportions(crosstab, 2) # Display column percentages female male Business 0.2714286 0.5719745 Leisure 0.7285714 0.4280255 chisq.test(crosstab) # Run test of independence Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: crosstab X-squared = 73.386, df = 1, p-value &lt; 2.2e-16 Table 2.4: Cross-tabs using Base R The look of the tables can be improved with the htmlTable package htmlTable(crosstab) female male Business 76 449 Leisure 204 336 htmlTable(round(proportions(crosstab,2)*100, 2)) female male Business 27.14 57.2 Leisure 72.86 42.8 Table 2.5: Cross-tabs using htmlTable 2.3.2.2 Alternative Packages The following packages are not availabe through the BGSU Virtual Computing lab, but can be installed if using R/RStudio on your own machine. These packages produce nicely formatted crosstabs. 2.3.2.2.1 Package sjPlot Use the function tab_xtab(var.row=, var.col=, show.col.prc=TRUE) to get a standard crosstab with column percentages library(sjPlot) tab_xtab(var.row=airlinesat$flight_purpose, var.col=airlinesat$gender, show.col.prc=TRUE) flight_purpose gender Total female male Business 7627.1 % 44957.2 % 52549.3 % Leisure 20472.9 % 33642.8 % 54050.7 % Total 280100 % 785100 % 1065100 % 2=73.386 · df=1 · =0.265 · p=0.000 Table 2.6: Cross-tab using sjPlot 2.3.2.2.2 Package gmodels FunctionCrossTable(rowvar, colvar, OPTIONS) has many options similar to SPSS library(gmodels) CrossTable(airlinesat$flight_purpose, airlinesat$gender, prop.r=FALSE, # Exclude row percentages prop.t=FALSE, # Exclude total percentages, prop.chisq=FALSE, # Exclude cell contribution to chi-sq digits=2, # 2 digits after decimal point chisq=TRUE, # Request test of independence format=&quot;SPSS&quot;) # Request SPSS formatting Cell Contents |-------------------------| | Count | | Column Percent | |-------------------------| Total Observations in Table: 1065 | airlinesat$gender airlinesat$flight_purpose | female | male | Row Total | --------------------------|-----------|-----------|-----------| Business | 76 | 449 | 525 | | 27.14% | 57.20% | | --------------------------|-----------|-----------|-----------| Leisure | 204 | 336 | 540 | | 72.86% | 42.80% | | --------------------------|-----------|-----------|-----------| Column Total | 280 | 785 | 1065 | | 26.29% | 73.71% | | --------------------------|-----------|-----------|-----------| Statistics for All Table Factors Pearson&#39;s Chi-squared test ------------------------------------------------------------ Chi^2 = 74.58406 d.f. = 1 p = 5.811064e-18 Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ------------------------------------------------------------ Chi^2 = 73.38648 d.f. = 1 p = 1.065938e-17 Minimum expected frequency: 138.0282 Table 2.7: Cross-tab using gmodels 2.3.3 Measures of Centrality and Dispersion 2.3.3.1 Base R Any individual summary statistic can be easily calculated using Base R with functions such as: mean(x) for mean sd(x) for standard deviation quantile(x, .percentile) for percentiles (e.g., .50 would be median) For summary statistics except for standard deviation, the summary(object) function can be used, where object can be a single variable or an entire data frame # Summary for a single variable summary(airlinesat$nflights) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 4.00 8.00 13.42 16.00 457.00 Table 2.8: Summary statistics in R Base, one variable # Subset of airlinesat summary(airlinesat[,c(&quot;age&quot;, &quot;nflights&quot;, &quot;s10&quot;)]) age nflights s10 Min. : 19.00 Min. : 1.00 Min. : 1.00 1st Qu.: 42.00 1st Qu.: 4.00 1st Qu.: 50.00 Median : 50.00 Median : 8.00 Median : 61.00 Mean : 50.42 Mean : 13.42 Mean : 64.54 3rd Qu.: 58.00 3rd Qu.: 16.00 3rd Qu.: 83.00 Max. :101.00 Max. :457.00 Max. :100.00 NA&#39;s :40 Table 2.9: Summary statistics in R Base, multiple variables Summary statistics for a continuous variable by different levels of a discrete variable can also be done in Base R using the tapply(continuous variable, discrete variable, function) function tapply(airlinesat$nflights, # Continous variable to apply the function to airlinesat$flight_purpose, # Discrete, grouping variable summary) # R function to apply by group $Business Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 6.00 12.00 18.65 25.00 120.00 $Leisure Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 3.000 4.000 8.337 8.000 457.000 Table 2.10: Summary statistics in R Base, one variable, grouped 2.3.3.2 Package dplyr The dplyr package can also be used to manually create tables of summary statistics One continuous variable airlinesat %&gt;% summarise(mean=mean(age), sd=sd(age), q1=quantile(age, .25), median=quantile(age,.50), q3=quantile(age, .75)) mean sd q1 median q3 1 50.41972 12.27464 42 50 58 Table 2.11: Summary statistics using dplyr, one variable Multiple continuous variables airlinesat %&gt;% select(age, nflights, s10) %&gt;% summary() age nflights s10 Min. : 19.00 Min. : 1.00 Min. : 1.00 1st Qu.: 42.00 1st Qu.: 4.00 1st Qu.: 50.00 Median : 50.00 Median : 8.00 Median : 61.00 Mean : 50.42 Mean : 13.42 Mean : 64.54 3rd Qu.: 58.00 3rd Qu.: 16.00 3rd Qu.: 83.00 Max. :101.00 Max. :457.00 Max. :100.00 NA&#39;s :40 Table 2.12: Summary statistics using dplyr, multiple variables One continuous variable by a discrete/grouping variable airlinesat %&gt;% group_by(flight_purpose) %&gt;% summarise(mean=mean(age), sd=sd(age), q1=quantile(age, .25), median=quantile(age,.50), q3=quantile(age, .75)) # A tibble: 2 x 6 flight_purpose mean sd q1 median q3 &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Business 48.5 9.91 41 49 55 2 Leisure 52.3 14.0 43 53 63 Table 2.13: Summary statistics using dplyr, one variable, grouped 2.3.3.3 Package vtable Package vtable produces very nice looking tables of summary statistics, but it isnt available in BGSUs Virtual Computer Lab. Use function sumtable(data, vars=\"varname\") to produce the table One continuous variable library(vtable) sumtable(airlinesat, vars=&quot;nflights&quot;) Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max nflights 1065 13.419 20.226 1 4 16 457 Table 2.14: Summary statistics using vtable, one variable Multiple continuous variables sumtable(airlinesat, vars=c(&quot;nflights&quot;,&quot;age&quot;,&quot;s10&quot;), # Use `c()` for multiple variables add.median=TRUE) # Request median Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 50 Pctl. 75 Max nflights 1065 13.419 20.226 1 4 8 16 457 age 1065 50.42 12.275 19 42 50 58 101 s10 1025 64.539 21.408 1 50 61 83 100 Table 2.15: Summary statistics using vtable, multiple variable One or more continuous variables by a discrete/grouping variable sumtable(airlinesat, vars=c(&quot;nflights&quot;,&quot;age&quot;,&quot;s10&quot;), add.median=TRUE, group=&quot;flight_purpose&quot;) Variable N Mean SD Median N Mean SD Median flight_purpose Business Leisure nflights 525 18.646 17.665 12 540 8.337 21.254 4 age 525 48.497 9.911 49 540 52.289 13.958 53 s10 511 61.955 21.14 60 514 67.107 21.384 65 Table 2.16: Summary statistics using vtable, multiple variables, grouped 2.3.4 Correlation Correlation provides a measure of the strength of association between two continuous variables. 2.3.4.1 Base R Base R can easily provide the correlation and a test of the correlation using the cor.test(variable1, variable2) function By default, it includes only observations that are non-missing in both variables cor.test(airlinesat$age, airlinesat$nflights) Pearson&#39;s product-moment correlation data: airlinesat$age and airlinesat$nflights t = -3.7998, df = 1063, p-value = 0.000153 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.17461941 -0.05608231 sample estimates: cor -0.115763 Table 2.17: Correlation with test in Base R Base R can also easily provide a correlation matrix of variables using the cor(data) function By default, correlation will only be calculated for those pairs of variables that have no missing values Use option use=\"pairwise.complete.obs\" to exclude observations that are non-missing in both variables However, Base R cannot produce a correlation matrix with p-values # First create data frame with only variables wanted mycorr &lt;- airlinesat[,c(&quot;age&quot;, &quot;nflights&quot;, &quot;s10&quot;)] # Use function `round` to limit to 3 digits after decimal point round(cor(mycorr, use=&quot;pairwise.complete.obs&quot;), 3) age nflights s10 age 1.000 -0.116 0.167 nflights -0.116 1.000 -0.121 s10 0.167 -0.121 1.000 Table 2.18: Correlation matrix in Base R 2.3.4.2 Package Hmisc The function rcorr() from the package Hmisc, which is available in the BGSU Virtual Computing Lab, can be used to create correlation matrices also The rcorr() function requires a matrix, so the data frame of variables must be coerced into a matrix first By default, rcorr() produces three separate matrices: correlation, number of observations, and p-values Separate tables can be requested rcorr(as.matrix(dataframe))]]\"r\"]] provides the correlation matrix rcorr(as.matrix(dataframe))]]\"P\"]] provides the matrix of p-values Separate tables can used with htmlTable() for nicer output library(Hmisc) rcorr(as.matrix(mycorr)) # NOTE: &#39;mycorr&#39; created in previous code age nflights s10 age 1.00 -0.12 0.17 nflights -0.12 1.00 -0.12 s10 0.17 -0.12 1.00 n age nflights s10 age 1065 1065 1025 nflights 1065 1065 1025 s10 1025 1025 1025 P age nflights s10 age 2e-04 0e+00 nflights 2e-04 1e-04 s10 0e+00 1e-04 Table 2.19: Correlation matrix using Hmisc # Use &#39;round()&#39; function to limit digits in output htmlTable(round(rcorr(as.matrix(mycorr))[[&quot;r&quot;]],4)) htmlTable(round(rcorr(as.matrix(mycorr))[[&quot;P&quot;]],5)) age nflights s10 age 1 -0.1158 0.1671 nflights -0.1158 1 -0.1206 s10 0.1671 -0.1206 1 age nflights s10 age 0.00015 0 nflights 0.00015 0.00011 s10 0 0.00011 Table 2.20: Separate correlation matrix output using Hmisc 2.3.4.3 Package sjPlot The function tab_corr() from the sjPlot package produces very nice correlation matrices sjPlot is not available in BGSUs Virtual Computing Lab library(sjPlot) tab_corr(mycorr, # Data frame of variables to use; created earlier na.deletion = &quot;pairwise&quot;, # Delete obs if either variable is missing corr.method = &quot;pearson&quot;, # Choose Pearson correlation coefficient show.p = TRUE, # Show asterisks for significant correlations digits = 3, # Show three decimal points triangle = &quot;lower&quot;, # Show only lower triangle fade.ns=FALSE) # Do not fade insignficant correlations) age nflights s10 age nflights -0.116*** s10 0.167*** -0.121*** Computed correlation used pearson-method with pairwise-deletion. Table 2.21: Correlation matrix output using sjPlot 2.3.4.4 Package GGally The ggpairs() function from package GGally can produce a combination scatterplot and correlation matrix library(GGally) ggpairs(mycorr, # Data frame created earlier lower=list(continuous=wrap(&quot;smooth&quot;, # Adds fit lines... method=&quot;lm&quot;, # Using linear regression... se=FALSE, # Without CI bands color=&quot;blue&quot;)), # Color dots diag=list(continuous=&quot;blankDiag&quot;)) # Sets diagonals to be blank Figure 2.23: Combination scatterplot/correlation matrix using GGally "],["linear-regression.html", "Chapter 3 Linear Regression 3.1 Introduction 3.2 The lm() Function 3.3 Prediction 3.4 Margin Plots", " Chapter 3 Linear Regression Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html ggplot2: https://ggplot2.tidyverse.org/ Data for this chapter: Once again, the airlinesat.rdata is used. Load it now. # You may need to change the directory load(&quot;Data/airlinesat.rdata&quot;) 3.1 Introduction Base R is typically sufficient for performing most regression tasks. Some additional packages may be used for prettier tables or extracting results to make useful plots. 3.2 The lm() Function Basic linear regression is performed using the lm() function. Usage: lm(formula, data) In R, a formula is represented by dependent variables on the left side separated from the independent variables on the right side by a tilde(~), such as: dv ~ iv1 + iv2 For interactions between independent variables, use either * or : * will include the interaction term AND each main effect : will include ONLY the iteraction term Examples: y ~ x1 + x2*x3 is the same as: \\(y=x_1+x_2+x_3+(x_2\\times x_3)\\) y ~ x1 + x2:x3 is the same as: \\(y=x_1+(x_2\\times x_3)\\) y ~ x1 + x2 + x2:x3 is the same as: \\(y=x_1+x_2+(x_2\\times x_3)\\) If lm() is run by itself, R only outputs the coefficients lm(nps ~ age + nflights, airlinesat) Call: lm(formula = nps ~ age + nflights, data = airlinesat) Coefficients: (Intercept) age nflights 6.786100 0.016985 -0.009112 Table 3.1: Coefficients from lm() call However, if the results of the lm() call are assigned to an object, the summary() function can be used to get much more detailed output model1 &lt;- lm(nps ~ age + nflights, airlinesat) summary(model1) Call: lm(formula = nps ~ age + nflights, data = airlinesat) Residuals: Min 1Q Median 3Q Max -6.988 -1.316 0.440 1.725 7.682 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.786100 0.310554 21.852 &lt; 2e-16 *** age 0.016985 0.005815 2.921 0.00357 ** nflights -0.009112 0.003529 -2.582 0.00995 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.313 on 1062 degrees of freedom Multiple R-squared: 0.01591, Adjusted R-squared: 0.01406 F-statistic: 8.587 on 2 and 1062 DF, p-value: 0.0001997 Table 3.2: Summary results from lm() call For nicer looking results, the package jtools can be used NOTE: jtools is no available in BGSUs Virtual Computing Lab library(jtools) summ(model1, # Saved object from before digits=4, # How many digits to display in each column model.info = FALSE) # Suppress extraneous information F(2,1062) 8.5875 R² 0.0159 Adj. R² 0.0141 Est. S.E. t val. p (Intercept) 6.7861 0.3106 21.8516 0.0000 age 0.0170 0.0058 2.9208 0.0036 nflights -0.0091 0.0035 -2.5822 0.0100 Standard errors: OLS Table 3.3: Summary results from lm() using jtools package To get standardized beta coefficients, use the lm_beta user defined function. source(&quot;lm_beta.R&quot;) # Download lm_beta.R and put in working directory lm_beta(model1, # Saved object with results digits=4) # Number of digits to diplsy Std.Beta (Intercept) 0.0000 age 0.0895 nflights -0.0791 Table 3.4: Standardized Beta Coefficients 3.3 Prediction The function predict.lm() can be used to predict the DV based on values of the IVs. This function is used in the margin plots covered in the next two sections. To use this function, we must pass a data frame of values to the function, where the data frame contains ALL of the IVs and the value for each IV that we want. Suppose we wanted to predict, with a confidence interval, the nps of someone that is 45 years old and had 25 flights on the airline, and also someone that is 25 years old and had 45 flights on the airline. First, we create the data frame of values (see 1.6.2.1): values &lt;- data.frame(age=c(45, 25), nflights=c(25, 45)) # Create data frame values # Verify values age nflights 1 45 25 2 25 45 Second the data frame is passed to the predict.lm() function with confidence intervals requested: predict.lm(model1, # The model we are using to predict values, # The data frame of values to predict with interval=&quot;confidence&quot;) # Request confidence interval fit lwr upr 1 7.322601 7.153951 7.491252 2 6.800657 6.431058 7.170255 3.4 Margin Plots In R, margin plots are a multistep process: Use the predictorEffect function from the effects package to create a data frame of predicted values for different values of the focal variable. For continuous focal variables, predictions will be made for 50 evenly-spaced values of the focal variable at mean values of the other variables. If the model has an interaction between a continuous focal variable and a factor variable, predictions will be made for each level of the factor variable. If the model has an interaction between a continuous focal variable and another continuous variable, predictions will be made for 5 evenly-spaced values of the other continuous variable. For factor focal variables, predictions will be made for each level of the factor variable at mean values of the other variables. If the model has an interaction between a factor focal variable and a continuous variable, predictions will be made for 5 evenly-spaced values of the the continuous variable. If the model has an interaction between a factor focal variable and another factor variable, predictions will be made for every combination of the two factor variables. By default, the dataframe will contain the predicted value (fit) and values for a \\(95\\%\\) confidence interval (lower and upper) Use ggplot to create the margin plot 3.4.1 Continuous IV (No Interaction) library(effects) # Load package &#39;effects&#39; # Want to predict &#39;nps&#39; for different levels of &#39;age&#39; age.pred &lt;- data.frame(predictorEffect(&quot;age&quot;, # Focal variable model1)) # Use &#39;age.pred&#39; for margin plot age.pred %&gt;% ggplot(aes(x=age, # Age on x-axis y=fit)) + # &#39;nps&#39; prediction on y-axis geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level labs(x=&quot;Age&quot;, y=&quot;Linear Prediciton&quot;) Figure 3.1: Margin plot (continuous IV) 3.4.2 Separate Margin Plots (No Interaction) If creating separate plots for each continuous IV, it is best to have the prediction (y) variable have the same scale Doing so might take some trial and error after looking at graphs once Use the function plot_grid() from the cowplot package to make the plots appear in the same figure Save each plot as an object, then pass the objects to plot_grid() library(cowplot) # Using dataframe from previous chunk, ask for min/max of the 95% CI adjust axis min(age.pred$lower) [1] 6.601996 max(age.pred$upper) [1] 8.944869 # Save previous plot (for &#39;age&#39;) as plot 1 plot1 &lt;- age.pred %&gt;% ggplot(aes(x=age,y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.2) + labs(x=&quot;Age&quot;, y=&quot;Linear Prediciton&quot;) + scale_y_continuous(limits=c(5.25,9.25)) # Set scale limits for y-axis # Create second plot for &#39;nflights&#39; using procedure above # Because of outliers in &#39;nflights&#39;, use the &#39;focal.levels=&#39; option for more # realistic predictions nflights.pred &lt;- data.frame(predictorEffect(&quot;nflights&quot;, # Focal variable model1, focal.levels=seq(0,150,3))) min(nflights.pred$lower) [1] 5.319659 max(nflights.pred$upper) [1] 7.809707 plot2 &lt;- nflights.pred %&gt;% ggplot(aes(x=nflights, y=fit)) + geom_line(size=1) + geom_ribbon(aes(ymin=lower,ymax=upper),alpha=0.2) + labs(x=&quot;Number of Flights&quot;, y=&quot;Linear Prediciton&quot;) + scale_y_continuous(limits=c(5.25,9.25)) # Set scale limits for y-axis # Use &#39;cowplot&#39; function &#39;plot_grid&#39; to combine plots cowplot::plot_grid(plot1,plot2) Figure 3.2: Separate margin plots (continuous factor IVs) 3.4.3 Factor IV (No Interaction) Margin plots for a factor IV without an interaction consist of a point and \\(95\\%\\) confidence interval error bars # Run new model with flight_purpose included model2 &lt;- lm(nps ~ age + nflights + flight_purpose, airlinesat) summary(model2) Call: lm(formula = nps ~ age + nflights + flight_purpose, data = airlinesat) Residuals: Min 1Q Median 3Q Max -7.130 -1.160 0.521 1.808 6.306 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.644742 0.313158 21.218 &lt; 2e-16 *** age 0.014749 0.005844 2.524 0.01175 * nflights -0.006540 0.003624 -1.805 0.07140 . flight_purposeLeisure 0.433007 0.147317 2.939 0.00336 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.304 on 1061 degrees of freedom Multiple R-squared: 0.02386, Adjusted R-squared: 0.0211 F-statistic: 8.646 on 3 and 1061 DF, p-value: 1.136e-05 Table 3.5: Summary results of model with factor variable # Create data frame with values to be plotted fp.pred &lt;- data.frame(predictorEffect(&quot;flight_purpose&quot;, model2)) # Create plot fp.pred %&gt;% ggplot(aes(x=flight_purpose, y=fit, group=1)) + # Need to draw line between points geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + labs(x=&quot;Flight Purpose&quot;, y=&quot;Linear Prediction&quot;) Figure 3.3: Margin plot (factor IV) 3.4.4 Continuous IV (Interaction with Factor IV) Margin plots when for a continuous IV interaction with a factor IV are done in much the same way, but require an aes(color=factor, fill=factor) to produce different colored lines for each level of the factor # Run new model with &#39;age&#39; and &#39;flight_purpose&#39; interaction model3 &lt;- lm(nps ~ age*flight_purpose + nflights, airlinesat) summary(model3) Call: lm(formula = nps ~ age * flight_purpose + nflights, data = airlinesat) Residuals: Min 1Q Median 3Q Max -6.9177 -1.0298 0.3193 1.8402 6.1721 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.675998 0.510101 11.127 &lt; 2e-16 *** age 0.034704 0.010148 3.420 0.000651 *** flight_purposeLeisure 1.912145 0.632942 3.021 0.002579 ** nflights -0.006487 0.003616 -1.794 0.073088 . age:flight_purposeLeisure -0.029724 0.012372 -2.403 0.016450 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.299 on 1060 degrees of freedom Multiple R-squared: 0.02915, Adjusted R-squared: 0.02549 F-statistic: 7.957 on 4 and 1060 DF, p-value: 2.551e-06 Table 3.6: Summary results of model with continuous IV and factor IV interaction # Create data frame with values to be plotted age.pred &lt;- data.frame(predictorEffect(&quot;age&quot;, model3)) # Create plot age.pred %&gt;% ggplot(aes(x=age, y=fit, color=flight_purpose, fill=flight_purpose)) + # color based on factor geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level labs(x=&quot;Age&quot;, y=&quot;Linear Prediciton&quot;, color=&quot;Flight Purpose&quot;, fill=&quot;Flight Purpose&quot;) + theme(legend.position=&quot;bottom&quot;) Figure 3.4: Margin plot (continuous IV with factor IV interaction) 3.4.5 Factor IV (Interaction with Continuous IV) Margin plots for a factor IV interaction with a continuous IV are done in much the same way, but require an facet_wrap(~continuousIV) to produce different different plots for evenly-spaced values of the continuous IV # Create data frame with values to be plotted # By default, 5 evenly-spaced values for &#39;age&#39; will be created, but for use # with &#39;facet_wrap&#39;, set &#39;xlevels=4&#39; fp.pred &lt;- data.frame(predictorEffect(&quot;flight_purpose&quot;, model3, xlevels=4)) # Create plot fp.pred %&gt;% ggplot(aes(x=flight_purpose, y=fit, group=1)) + geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + facet_wrap(~age) + labs(x=&quot;Flight Purpose&quot;, y=&quot;Linear Prediction&quot;) Figure 3.5: Margin plot (factor IV with continuous IV interaction) 3.4.6 Continuous IV (Interaction with Continuous IV) # Run new model with &#39;age&#39; and &#39;overall_sat&#39; interaction model4 &lt;- lm(nps ~ age*overall_sat + nflights, airlinesat) summary(model4) Call: lm(formula = nps ~ age * overall_sat + nflights, data = airlinesat) Residuals: Min 1Q Median 3Q Max -7.9218 -0.9129 0.4645 1.4897 5.7636 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.181885 0.748391 9.596 &lt; 2e-16 *** age -0.023019 0.014642 -1.572 0.11623 overall_sat -0.051154 0.177249 -0.289 0.77294 nflights -0.007328 0.003370 -2.175 0.02986 * age:overall_sat 0.009427 0.003448 2.734 0.00636 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.205 on 1060 degrees of freedom Multiple R-squared: 0.1068, Adjusted R-squared: 0.1035 F-statistic: 31.7 on 4 and 1060 DF, p-value: &lt; 2.2e-16 Table 3.7: Summary results of model with continuous IV and continuous IV interaction # Create data frame with values to be plotted # By default, 5 evenly-spaced values for &#39;overall_sat&#39; will be created, but for # easier interpretation, set &#39;xlevels=4&#39; age.pred &lt;- data.frame(predictorEffect(&quot;age&quot;, model4, xlevels=4)) # Create plot age.pred %&gt;% ggplot(aes(x=age, y=fit, color=factor(overall_sat), fill=factor(overall_sat))) + # color based on factor geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level labs(x=&quot;Age&quot;, y=&quot;Linear Prediciton&quot;, color=&quot;Overall Satisfaction&quot;, fill=&quot;Overall Satisfaction&quot;) + theme(legend.position=&quot;bottom&quot;) Figure 3.6: Margin plot (continuous IV with continuous IV interaction) "],["logistic-regression-binary.html", "Chapter 4 Logistic Regression (Binary) 4.1 Introduction 4.2 The glm() Function 4.3 Logistic Regression using glm() 4.4 Estimate with Training Sample Only 4.5 Margin Plots 4.6 Classification Matrix 4.7 ROC Curve 4.8 Gain/Lift Charts and Tables 4.9 Sensitivity/Specificity Plot", " Chapter 4 Logistic Regression (Binary) Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html ggplot2: https://ggplot2.tidyverse.org/ Data for this chapter: The directmktg.rdata is used. Load it now. # You may need to change the directory load(&quot;Data/directmktg.rdata&quot;) 4.1 Introduction Base R is typically sufficient for performing the basics of logisitic regression, but to get some of the outputs required, I have created some user defined functions. You should download these functions and save them in your working directory. or_table.R produces Odds Ratio Coefficients logreg_cm.R produces the Classification Matrix logreg_roc.R produces the ROC Curves gainlift.R produces Gain/Lift tables and charts logreg_cut.R produces the Sensitivity/Specificity Plot Once saved in your working directory, it is good practice to source them. source(&quot;or_table.R&quot;) source(&quot;logreg_cm.R&quot;) source(&quot;logreg_roc.R&quot;) source(&quot;gainlift.R&quot;) source(&quot;logreg_cut.R&quot;) 4.2 The glm() Function glm stands for Generalized Linear Model, and this function can be used with a variety of dependent variables by specifying different family=\"FAMILY\" options *lm(dv ~ iv1 + iv2, data) is the same as `glm(dv ~ iv1 + iv2, data, family=gaussian) Usage: glm(formula, data, family) 4.3 Logistic Regression using glm() Binary logistic regression is performed using the glm() function when the family=\"binomial\" is specified. Usage: glm(formula, data, family=\"binonmial\") family=\"binomial\" tells R to use logistic regression on a binary dependent variable If glm(formula, data, family=\"binomial\") is run by itself, R only outputs the **Logit formulation* coefficients (and some other measures) glm(buy ~ age + gender, directmktg, family=&quot;binomial&quot;) Call: glm(formula = buy ~ age + gender, family = &quot;binomial&quot;, data = directmktg) Coefficients: (Intercept) age genderFemale -8.02069 0.18954 -0.09468 Degrees of Freedom: 399 Total (i.e. Null); 397 Residual Null Deviance: 521.6 Residual Deviance: 336.1 AIC: 342.1 Table 4.1: Logit Coefficients from glm() call However, if the results of the glm() call are assigned to an object, the Base R summary() function can be used to get much more detailed output, but requires manual calculation of McFaddens Pseudo-\\(R^2\\) prelim &lt;- glm(buy ~ age + gender, directmktg, family=&quot;binomial&quot;) summary(prelim) Call: glm(formula = buy ~ age + gender, family = &quot;binomial&quot;, data = directmktg) Deviance Residuals: Min 1Q Median 3Q Max -2.4952 -0.6679 -0.2844 0.5748 2.4684 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -8.02069 0.78700 -10.191 &lt;2e-16 *** age 0.18954 0.01926 9.842 &lt;2e-16 *** genderFemale -0.09468 0.27354 -0.346 0.729 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 521.57 on 399 degrees of freedom Residual deviance: 336.14 on 397 degrees of freedom AIC: 342.14 Number of Fisher Scoring iterations: 5 # Manually calculate McFadden&#39;s Pseudo R-sq Mrsq &lt;- 1-prelim$deviance/prelim$null.deviance cat(&quot;McFadden&#39;s Pseudo-Rsquared = &quot;, Mrsq) McFadden&#39;s Pseudo-Rsquared = 0.3555239 Table 4.2: Summary results from glm() call For nicer looking results, the package jtools can be used NOTE: jtools is not available in BGSUs Virtual Computing Lab library(jtools) summ(prelim, # Saved object from before digits=4, # How many digits to display in each column model.info = FALSE) # Suppress extraneous information ²(2) 185.4317 Pseudo-R² (Cragg-Uhler) 0.5092 Pseudo-R² (McFadden) 0.3555 AIC 342.1413 BIC 354.1157 Est. S.E. z val. p (Intercept) -8.0207 0.7870 -10.1915 0.0000 age 0.1895 0.0193 9.8422 0.0000 genderFemale -0.0947 0.2735 -0.3461 0.7293 Standard errors: MLE Table 4.3: Summary results from glm() using jtools package To get the Odds Ratio coefficients, use the user defined function or_table.R Usage: or_table(MODEL, DIGITS, CONF) MODEL is the name of the object with results of the glm() call DIGITS is the number of digits to round the values to in the table CONF is the confidence interval level (e.g., 95 = 95%) flextable( # Wrapping it with &#39;flextable&#39; for nicer output or_table(prelim, # Saved logistic regression model from above 4, # Number of digits to round output to 95)) # Level of confidence .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-5543a43e{}.cl-553d190c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-553d190d{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-553d190e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-553d66fa{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-553d66fb{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-553d66fc{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-553d66fd{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-553d66fe{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-553d66ff{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} ParameterOR Estp2.5%97.5%(Intercept)0.00030.00000.00010.0015age1.20870.00001.16391.2552genderFemale0.90970.72930.53221.5550 Table 4.4: Odds Ratio Coefficients 4.4 Estimate with Training Sample Only Generally, it is good practice to use a training sample and a testing/holdout sample to see how well the model performs out of sample. To do this, the data must be split into two groups: train and test 4.4.1 Creating train and test Samples While this can be done in a number of ways, the package caret has a very useful function that attempts to balance the percent of positive cases in both samples, while still creating random samples After running this procedure, two new data frames will be created One containing the training data, train One containing the testing/holdout data, test # Use &#39;caret&#39; package to create training and test/holdout samples # This will create two separate dataframes: train and test library(caret) # Load the &#39;caret package&#39; # Start by setting a random number seed so that the same samples # will be created each time set.seed(4320) # Create a dataframe of row numbers that should be in the &#39;train&#39; sample inTrain &lt;- createDataPartition(y=directmktg$buy, # Outcome variable p=.75, # Percent of sample to be in &#39;train&#39; list=FALSE) # Put result in matrix # Select only rows from data that are in &#39;inTrain&#39;; assign to &#39;train&#39; train &lt;- directmktg[inTrain,] # Select only rows from data that not in &#39;inTrain&#39;; assign to &#39;test&#39; test &lt;- directmktg[-inTrain,] 4.4.2 Run Model with only Training Sample Once completed, run the model on the training sample only model &lt;- glm(buy ~ age + gender, train, family=&quot;binomial&quot;) summ(model, # &#39;jtools&#39; results; use &#39;summary&#39; for Base R 4, model.info=FALSE) ²(2) 130.57 Pseudo-R² (Cragg-Uhler) 0.48 Pseudo-R² (McFadden) 0.33 AIC 268.37 BIC 279.49 Est. S.E. z val. p (Intercept) -7.71 0.87 -8.81 0.00 age 0.18 0.02 8.52 0.00 genderFemale -0.01 0.31 -0.04 0.97 Standard errors: MLE; Continuous predictors are mean-centered and scaled by 1 s.d. flextable(or_table(model)) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-a11e5c22{}.cl-a1181db2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a11845d0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a11845d1{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a1186ce0{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a1186ce1{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a1186ce2{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a1186ce3{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a1186ce4{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a1186ce5{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} ParameterOR Estp2.5%97.5%(Intercept)0.00040.0000.00010.0025age1.19960.0001.15051.2509genderFemale0.98730.9670.53991.8056 Table 4.5: Logistic Regression Results for Training Sample 4.5 Margin Plots Margin plots are done in much the same way as with linear regression margin plots # Load &#39;effects&#39; package library(effects) # Create data frame with values to be plotted with &#39;age&#39; as focal age.pred &lt;- data.frame(predictorEffect(&quot;age&quot;, model)) # Create plot and assign to &#39;p1&#39; p1 &lt;- age.pred %&gt;% ggplot(aes(x=age, y=fit)) + geom_line(size=1) + # Draw predicted line geom_ribbon(aes(ymin=lower, # Draws the confidence interval bands ymax=upper), alpha=0.2) + # Sets transparency level labs(x=&quot;Age&quot;, y=&quot;Pr(Buy)&quot;) + scale_y_continuous(limits=c(0,1)) # Create data frame with values to be plotted with &#39;gender&#39; as focal g.pred &lt;- data.frame(predictorEffect(&quot;gender&quot;, model)) # Create plot and assign to &#39;p2&#39; p2 &lt;- g.pred %&gt;% ggplot(aes(x=gender, y=fit, group=1)) + # Need to draw line between points geom_point(size=4) + geom_line(color=&quot;orange&quot;) + geom_errorbar(aes(ymin=lower, ymax=upper), width=.5) + labs(x=&quot;Gender&quot;, y=&quot;Pr(Buy)&quot;) + scale_y_continuous(limits=c(0,1)) #Use &#39;cowplot&#39; to put into single plot plot_grid(p1,p2) Figure 4.1: Margin plots for variables in model 4.6 Classification Matrix To get the Classification Matrix, use the user defined function logreg_cm.R Usage: logreg_cm(MOD, DATA, \"POS\", CUTOFF=) MOD is the name of the object with results of the glm() call DATA is the data set for which the Classification Matrix should be produced (i.e., original, training, or testing) \"POS\" is the level of the factor variable that is the SUCCESS level CUTOFF= is the cutoff threshold; default is 0.5 This function requires the package caret, which should already be loaded from the splitting of the data # Classification Matrix for training sample logreg_cm(model, # Name of the results object train, # Use training sample &quot;Yes&quot;) # Level of &#39;buy&#39; that represents success/true Confusion Matrix and Statistics Reference Prediction No Yes No 179 37 Yes 14 71 Accuracy : 0.8306 95% CI : (0.7833, 0.8712) No Information Rate : 0.6412 P-Value [Acc &gt; NIR] : 3.179e-13 Kappa : 0.6136 Mcnemar&#39;s Test P-Value : 0.002066 Sensitivity : 0.6574 Specificity : 0.9275 Pos Pred Value : 0.8353 Neg Pred Value : 0.8287 Prevalence : 0.3588 Detection Rate : 0.2359 Detection Prevalence : 0.2824 Balanced Accuracy : 0.7924 &#39;Positive&#39; Class : Yes PCC = 53.99% Table 4.6: Classification Matrix for Training Sample 4.7 ROC Curve To get the ROC curve, use the user defined function logreg_roc.R Usage: logreg_roc(MOD, DATA) MOD is the name of the object with results of the glm() call DATA is the data set for which the ROC should be produced (i.e., original, training, or testing) This function requires the package pROC, which needs to be loaded # Load &#39;pROC&#39; package library(pROC) # ROC Curve for the training sample logreg_roc(model, # Name of the results object train) # Use training sample Figure 4.2: ROC Curve for Training Sample 4.8 Gain/Lift Charts and Tables To get the Gain/Lift charts and tables , use the user defined function gainlift.R Usage: OBJ.NAME &lt;- gainlift(MOD, TRAIN, TEST, \"POS\") OBJ.NAME is the name of the object to assign the results to MOD is the name of the object with results of the glm() call TRAIN is the name of the data frame containing the training sample TEST is the name of the data frame containing the test/holdout sample \"POS\" is the level of the factor variable that is the SUCCESS level This function requires the ggplot2, dplyr, and tidyr packages, which need to be loaded first This user defined function returns a list of four objects To use, assign the result to an object name, and then call one of the four objects returned from the function # Load necessary packages (if not already loaded) library(ggplot2) library(dplyr) library(tidyr) # Call the function and assign to object named &#39;glresults&#39; glresults &lt;- gainlift(model, # Name of the glm results object train, # Name of the training data frame test, # Name of the testing data frame &quot;Yes&quot;) # Level that represents success/true OBJ.NAME$gaintable returns the Gain table glresults$gaintable # A tibble: 20 x 3 `% Sample` Holdout Training &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.05 0.114 0.130 2 0.1 0.229 0.269 3 0.15 0.371 0.370 4 0.2 0.457 0.5 5 0.25 0.6 0.602 6 0.3 0.714 0.667 7 0.35 0.8 0.704 8 0.4 0.8 0.731 9 0.45 0.829 0.769 10 0.5 0.886 0.815 11 0.55 0.914 0.861 12 0.6 0.943 0.870 13 0.65 0.971 0.889 14 0.7 1 0.944 15 0.75 1 0.954 16 0.8 1 0.972 17 0.85 1 0.991 18 0.9 1 1 19 0.95 1 1 20 1 1 1 Table 4.7: Gain Table OBJ.NAME$gainplot returns the Gain plot glresults$gainplot Figure 4.3: Gain Plot OBJ.NAME$lifttable returns the Lift table glresults$lifttable # A tibble: 20 x 3 `% Sample` Holdout Training &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.05 2.83 2.60 2 0.1 2.51 2.69 3 0.15 2.63 2.48 4 0.2 2.38 2.51 5 0.25 2.48 2.42 6 0.3 2.44 2.23 7 0.35 2.33 2.02 8 0.4 2.03 1.83 9 0.45 1.86 1.71 10 0.5 1.79 1.64 11 0.55 1.68 1.57 12 0.6 1.58 1.46 13 0.65 1.50 1.37 14 0.7 1.43 1.35 15 0.75 1.34 1.28 16 0.8 1.25 1.22 17 0.85 1.18 1.17 18 0.9 1.11 1.11 19 0.95 1.05 1.06 20 1 1 1 Table 4.8: Lift Table OBJ.NAME$liftplot returns the Lift plot glresults$liftplot Figure 4.4: Lift Plot 4.9 Sensitivity/Specificity Plot To get the Sensitivity/Specificity plot, use the user defined function logreg_cut.R Usage: logreg_cut.r(MOD, DATA, \"POS\") MOD is the name of the object with results of the glm() call DATA is the data set for which the plot should be produced (i.e., original, training, or testing) \"POS\" is the level of the factor variable that is the SUCCESS level This function requires the ggplot2 package, which needs to be loaded first # Load necessary packages library(ggplot2) # Sensitivity/Specificity Plot for Training Sample logreg_cut(model, # Name of the results object train, # Use training sample &quot;Yes&quot;) # Level of &#39;buy&#39; that represents success/true Figure 4.5: Sensitivity/Specificity Plot for Training Sample "],["cluster-analysis.html", "Chapter 5 Cluster Analysis 5.1 Introduction 5.2 Preparation 5.3 Hierarchical Agglomerative Clustering 5.4 k-Means Clustering 5.5 Describing clusters", " Chapter 5 Cluster Analysis Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html Data for this chapter: The ffseg.rdata is used. Load it now. # You may need to change the directory load(&quot;Data/ffseg.rdata&quot;) 5.1 Introduction Base R is typically sufficient for performing the basics of both hierarchical and k-means cluster analysis, but to get some of the outputs required more easily and more efficiently, I have created some user defined functions. You should download these functions and save them in your working directory. clustop.R provides Duda-Hart and psuedo-\\(T^2\\) indices for hierarchical agglomerative clustering cldescr.R describes variables based on cluster membership myhc.R produces a dendrogram and cluster sizes and percents for hierarchical agglomerative clustering wssplot.R produces a scree plot for \\(k\\)-means ksize.R produces cluster size and proportion tables for \\(k\\)-means kcenters.R produces cluster center table and plot from \\(k\\)-means Once saved in your working directory, it is good practice to source them. source(&quot;clustop.R&quot;) source(&quot;cldescr.R&quot;) source(&quot;myhc.R&quot;) source(&quot;wssplot.R&quot;) source(&quot;ksize.R&quot;) source(&quot;kcenters.R&quot;) 5.2 Preparation For both hierarchical agglomerative and k-means clustering, we usually prepare our data for clustering In other words, choose which variable to use for clustering Package dplyr is usually the best tool for this library(dplyr) # Store variables selected using dplyr to &#39;segvar&#39; clvar &lt;- ffseg %&gt;% select(quality, price, healthy, variety, speed) * To standardize the clustering variables, use the `scale` function * Usage: `scale(data)` * To store as a data frame, wrap the command in `data.frame()` # Standardize &#39;clvar&#39; df and store as &#39;sc.clvar&#39; sc.clvar &lt;- data.frame(scale(clvar)) 5.3 Hierarchical Agglomerative Clustering 5.3.1 Base R In Base R, hierarchical cluster analysis is done in multiple steps, Use the dist() function to create a similarity/dissimilarity matrix Use the hclust() function to perform hierarchical cluster analysis on the matrix from (1) Determine how many clusters are desired using a dendrogram and/or the stopping indices from package NbClust 5.3.1.1 Creating the similarity/dissimilarity matrix Usage: dist(data, method=\"\") where: data is the (scaled) cluster variable data method=\"\" indicates the distance measure: method=\"euclidean\" provides Euclidean distance method=\"maximum\" provides Maximum distance method=\"manhattan\" provides Absolute distance method=\"binary\" provides a distance measure for a set of binary-only variables NOTE: other choices exist, but for MKT 4320, our focus is on these four # Create the similarity/dissimilarity matrix and save as object &#39;dist&#39; dist &lt;- dist(sc.clvar, # Scaled data from earlier method=&quot;euclidean&quot;) # Euclidean distance measure 5.3.1.2 Perform hierarchical clustering Usage: hclust(d, method=\"\") where: *dis the similarity/dissimilarity matrix *method=\"indicates the linkage method: *method=singleprovides Single linkage *method=completeprovides Complete linkage *method=averageprovides Average linkage *method=ward.D\"` provides Wards linkage * NOTE: other choices exist, but for MKT 4320, our focus is on these four # Preform hierarchical clustering and save as object &#39;hc&#39; hc &lt;- hclust(dist, # similarity/dissimilarity matrix from earlier method=&quot;ward.D&quot;) # Ward&#39;s linkage method # NOTE: The similarity/dissimilarity matrix can be done within the call # to &#39;hclust&#39; hc1 &lt;- hclust(dist(sc.clvar, method=&quot;euclidean&quot;), method=&quot;ward.D&quot;) 5.3.1.3 Create a dendrogram Usage: plot(x) where: x is a hierarchical clustering object # Create dendrogram of object hc plot(hc) 5.3.1.4 Stopping indices To get the Duda-Hart index, and the pseudo-\\(T^2\\), the NbClust package is used This package is not available in BGSUs Virtual Computing Lab Usage: NbClust(data, distance=\"\", method=\"\", min.nc=, max.nc=, index=\"\")$All.index where: data is the (scaled) cluster variable data distance=\"\" indicates the distance measure using the same options from the dist function above method=\"\" indicates the linkage method using the same options from the hclust option above min.nc= and max.nc= indicate the minimum and maximum number of clusters to examine index=\"\" indicates what measure is wanted: index=\"duda\" uses the Duda-Hart index index=\"pseudot2 uses the pseudo-\\(T^2\\) index $All.index requests only the index values be returned # After installing package for the first time, load the &#39;NbClust&#39; package library(NbClust) # Get Duda-Hart Index NbClust(sc.clvar, # Scaled cluster variable data from earlier distance=&quot;euclidean&quot;, # Euclidean distance measure method=&quot;ward.D&quot;, # Ward&#39;s linkage min.nc=1, # Show between 1 and... max.nc=10, # ... 10 clusters index=&quot;duda&quot;)$All.index # Request Duda-Hart index 1 2 3 4 5 6 7 8 9 10 0.7974 0.8367 0.8435 0.8490 0.7448 0.8628 0.8222 0.8206 0.6950 0.6470 # Get pseudo-T2 NbClust(sc.clvar, distance=&quot;euclidean&quot;, method=&quot;ward.D&quot;, min.nc=1, max.nc=10, index=&quot;pseudot2&quot;)$All.index 1 2 3 4 5 6 7 8 190.5756 88.3818 54.7376 50.5151 58.5770 41.3604 36.1142 40.0002 9 10 46.9583 40.9146 5.3.2 User Defined Functions The myhc.R user defined function can produce the results with one or two passes of the function Requires the following packages: dendextend dplyr NbClust The results should be saved to an object Usage: myhc(data, dist=\"\", method=\"\", cuts, clustop=\"\") where: data is the (scaled) cluster variable data dist=\"\" indicates the distance measure: dist=\"euc\" provides Euclidean distance dist=\"euc2\" provides Euclidean Squared distance dist=\"max\" provides Maximum distance dist=\"abs\" provides Absolute distance dist=\"bin\" provides a distance measure for a set of binary-only variables method=\"\" indicates the linkage method: method=\"single\" provides Single linkage method=\"complete\" provides Complete linkage method=\"average\" provides Average linkage method=\"ward\" provides Wards linkage cuts indicates how many clusters (optional): Examples: cuts=3; cuts=c(2,4,5) clustop=\"\" indicates if stopping indices are wanted clustop=\"Y\" if indices are wanted clustop=\"N\" if indices are not wanted Objects returned: If cuts are provided: Dendrogram of the top \\(n\\) branches, where \\(n\\) is the highest number of clusters provided by cuts Table of cluster sizes ($kcount) Table of cluster size percentages ($kperc) hclust object ($hc) Stopping indices for 1 to 10 clusters, if requested ($stop) If cuts are not provided: Dendrogram with all branches Stopping indices for 1 to 10 clusters, if requested ($stop) Examples: No cuts with stopping indices # Both myhc.R and clustop.R need to be &quot;sourced&quot; eg1 &lt;- myhc(sc.clvar, &quot;euc&quot;, &quot;ward&quot;, clustop=&quot;Y&quot;) Loading required package: dendextend --------------------- Welcome to dendextend version 1.15.2 Type citation(&#39;dendextend&#39;) for how to cite the package. Type browseVignettes(package = &#39;dendextend&#39;) for the package vignette. The github page is: https://github.com/talgalili/dendextend/ Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues You may ask questions at stackoverflow, use the r and dendextend tags: https://stackoverflow.com/questions/tagged/dendextend To suppress this message use: suppressPackageStartupMessages(library(dendextend)) --------------------- Attaching package: &#39;dendextend&#39; The following object is masked from &#39;package:flextable&#39;: rotate The following object is masked from &#39;package:stats&#39;: cutree # Dendrogram will get produced automatically # Call eg1$stop to get table of stopping indices eg1$stop Num.Clusters Duda.Hart pseudo.t.2 1 1 0.7974 190.5756 2 2 0.8367 88.3818 3 3 0.8435 54.7376 4 4 0.8490 50.5151 5 5 0.7448 58.5770 6 6 0.8628 41.3604 7 7 0.8222 36.1142 8 8 0.8206 40.0002 9 9 0.6950 46.9583 10 10 0.6470 40.9146 One cut without stopping indices eg2 &lt;- myhc(sc.clvar, &quot;max&quot;, &quot;ward&quot;, cuts=5, clustop=&quot;N&quot;) # Dendrogram will get produced automatically # call eg2$kcount and eg2$kperc to get cluster sizes eg2$kcount Cluster k_5_Count 1 1 244 2 2 229 3 3 127 4 4 91 5 5 61 eg2$kperc Cluster k_5_Percent 1 1 32.45 2 2 30.45 3 3 16.89 4 4 12.10 5 5 8.11 Multiple cuts without stopping indices eg3 &lt;- myhc(sc.clvar, &quot;abs&quot;, &quot;ward&quot;, cuts=c(2,3,4,6), clustop=&quot;N&quot;) # Dendrogram will get produced automatically # call eg3$kcount and eg3$kperc to get cluster sizes eg3$kcount Cluster k_2_Count k_3_Count k_4_Count k_6_Count 1 1 537 344 215 205 2 2 215 215 205 153 3 3 NA 193 193 139 4 4 NA NA 139 120 5 5 NA NA NA 73 6 6 NA NA NA 62 eg3$kperc Cluster k_2_Percent k_3_Percent k_4_Percent k_6_Percent 1 1 71.41 45.74 28.59 27.26 2 2 28.59 28.59 27.26 20.35 3 3 NA 25.66 25.66 18.48 4 4 NA NA 18.48 15.96 5 5 NA NA NA 9.71 6 6 NA NA NA 8.24 5.3.3 Cluster membership Use the cutree() function to get cluster membership for the specified number of clusters Usage: cutree(tree, k=) where: tree is an hclust object from Base R methods or from myhc function k= is the number of clusters to retrieve Assign cluster membership to original data, cluster variables, or both # Create new variables in original data based on cluster membership # Need to use &#39;as.factor()&#39; to classify variable as factor ffseg$K3 &lt;- as.factor(cutree(hc, # &#39;hclust&#39; object from Base R earlier k=3)) # Number of clusters to retrieve ffseg$K4 &lt;- as.factor(cutree(eg3$hc, # &#39;hclust&#39; object from call to &#39;myhc&#39; earlier k=4)) # Number of clusters to retrieve # Create new data frame of scaled clustering variables # with cluster membership appended # &#39;cbind&#39; stands for column bind sc.cldesc &lt;- cbind(sc.clvar, # Scaled cluster variables K3=as.factor(cutree(hc, k=3))) # Cluster membership variable 5.4 k-Means Clustering 5.4.1 Base R \\(k\\)-Means clustering is done mostly in Base R, but there is a couple user defined functions to help Process: Run wssplot.R user defined function to get a scree plot Based on scree plot, use ksize() user defined function to examine the cluster sizes for several solutions Run kmeans() for desired solution Assign cluster membership for desired number of clusters to original data, cluster variables, or both 5.4.1.1 Scree plot The wssplot.R user defined function produces a scree plot for 1 to \\(n\\) clusters Requires the ggplot2 package Usage: wssplot(data, nc=, seed=) where: data is the (scaled) cluster variable data nc= takes an integer value and is the maximum number of clusters to plot; default is 15 seed= is a random number seed for reproducible results; default is 4320 # &#39;wssplot.R&#39; must first be sourced (if not already) wssplot(sc.clvar) # Produces scree plot with default 15 clusters and 4320 seed 5.4.1.2 Cluster sizes The ksize.R user defined function produces a count and proportion tables for the selected \\(k\\) solutions The results should be saved to an object Usage: ksize(data, centers=, nstart=, seed=) where: data is the (scaled) cluster variable data centers= takes one or more integers for the \\(k\\) cluster solutions to be examined nstart= takes an integer value for the number of random starting points to try; default is 25 seed= is a random number seed for reproducible results; default is 4320 Objects returned: Table of cluster sizes ($kcount) Table of cluster size percentages ($kperc) # &#39;ksize.R&#39; must first be sourced (if not already) ks &lt;- ksize(sc.clvar, # Scaled cluster variables from earlier centers=c(3,4,5), # Request for 3, 4, and 5 cluster solutions nstart=25, # Request 25 random starting sets seed=4320) # Set seed to 4320 for reproducible results ks$kcount # Cluster sizes Num_Clusters k_3_Count k_4_Count k_5_Count 1 1 308 211 205 2 2 238 191 167 3 3 206 187 150 4 4 NA 163 124 5 5 NA NA 106 ks$kperc # Cluster percentages Num_Clusters k_3_Percent k_4_Percent k_5_Percent 1 1 40.96 28.06 27.26 2 2 31.65 25.40 22.21 3 3 27.39 24.87 19.95 4 4 NA 21.68 16.49 5 5 NA NA 14.10 5.4.1.3 kmeans function Usage: kmeans(data, centers=, nstart=) where: data is the (scaled) cluster variable data centers= takes an integer value for the number of clusters desired nstart= takes an integer value for the number of random starting points to try Results should be assigned to an object NOTE 1: For reproducible results, set.seed() should be run immediately before running kmeans() NOTE 2: nstart= should be same value as above Use the saved $cluster object to assign cluster membership set.seed(4320) # Set random number seed # Run kmeans() for 4 clusters and assign to object k3 k4 &lt;- kmeans(sc.clvar, # Scaled cluster variables from earlier centers=4, # Request 3-cluster solution nstart=25) # Request 25 random starting sets ffseg$KM4 &lt;- as.factor(k4$cluster) 5.5 Describing clusters 5.5.1 cldescr.R The cldescr.R user defined function can be used to describe the clusters on both cluster variables and other variables that may be in the dataset The results should be assigned to an object Usage: cldescr(data, var=\"\", vtype=c(\"F\", \"C\"), cvar=\"\" where: data is the original data or (scaled) cluster variable data with cluster membership attached var=\"\" contains the name of the variable to examine by cluster membership vtype=\"\" takes F if var is a factor variable or C if var is a continuous variable cvar=\"\" contains the name of the cluster membership variable If vtype=\"F\" with more than two levels, the factor variable is split into dummy variables (1=True, 0=False) for each level of the factor If vtype=\"F\" with only two levels, the only the first level of the factor variable is shown as a dummy variable (1=True, 0=False) If vtype=\"C\" a list (using c()) of variables can be provided Objects returned Table of means ($means) If vtype=\"F\", one column per level of factor Table of ANOVA p-values ($aovp) Table(s) of Tukey HSD multiple comparisons for any variables where the ANOVA p-value\\(&lt;0.1\\) (tukey) 5.5.2 kcenters.R For \\(k\\)-Means, the kcenters.R user defined function can be used to describe the cluster centers Requires ggplot2 package The results should be assigned to an object Usage: kcenters(kobject) where: kobject is the name of a saved \\(k\\)-means object Objects returned Table of means ($table) ggplot object ($plot) 5.5.3 Examples Describing hierarchical clustering using clustering variables # &#39;cldescr.R&#39; must first be sourced (done above) hc.cv &lt;- cldescr(sc.cldesc, # Scaled cluster variables w/ cluster membership var=c(&quot;quality&quot;, &quot;price&quot;, &quot;healthy&quot;, # List of cluster variables &quot;variety&quot;, &quot;speed&quot;), vtype=&quot;C&quot;, # All variables are continuous cvar=&quot;K3&quot;) # Cluster membership variable hc.cv$means Cluster quality price healthy variety speed 1 1 0.7033 0.0515 0.7903 0.5729 0.3159 2 2 -0.5419 0.5722 -0.5865 -0.3641 0.0258 3 3 -0.3189 -1.0589 -0.3964 -0.3907 -0.5989 hc.cv$aovp Variable p.value 1 quality 0 2 price 0 3 healthy 0 4 variety 0 5 speed 0 # hc.cv$tukey # NOT SHOWN BECAUSE USUALLY ALL SIGNIFICANTLY DIFFERENT Describing hierarchical clustering or \\(k\\)-means using non-clustering continuous variables done the same as previous example Describing hierarchical clustering or \\(k\\)-means using non-clustering factor variables # Factor variable &#39;live&#39; on k-means solution km.fv &lt;- cldescr(ffseg, var=&quot;live&quot;, vtype=&quot;F&quot;, cvar=&quot;KM4&quot;) km.fv$means Cluster Commuter Off.Campus On.Campus 1 1 0.1123 0.4545 0.4332 2 2 0.0613 0.4417 0.4969 3 3 0.1623 0.3665 0.4712 4 4 0.0758 0.4929 0.4313 km.fv$aovp Variable p.value 1 Commuter 0.0069 2 Off.Campus 0.0805 3 On.Campus 0.5349 km.fv$tukey $Commuter diff p adj 2-1 -0.0509 0.3977 3-1 0.0500 0.3776 4-1 -0.0365 0.6286 3-2 0.1010 0.0101 4-2 0.0145 0.9681 4-3 -0.0865 0.0229 $Off.Campus diff p adj 2-1 -0.0128 0.9950 3-1 -0.0881 0.3101 4-1 0.0383 0.8677 3-2 -0.0752 0.4848 4-2 0.0512 0.7550 4-3 0.1264 0.0528 # Factor variable &#39;gender&#39; on hierarchcial clustering solution hc.fv &lt;- cldescr(ffseg, var=&quot;gender&quot;, vtype=&quot;F&quot;, cvar=&quot;K3&quot;) hc.fv$means Cluster Female 1 1 0.8182 2 2 0.6783 3 3 0.5917 hc.fv$aovp Variable p.value 1 Female 0 hc.fv$tukey diff p adj 2-1 -0.1399 0.0004 3-1 -0.2265 0.0000 3-2 -0.0866 0.1100 Describing \\(k\\)-means clustering using cluster centers km.cent &lt;- kcenters(k4) km.cent$table quality price healthy variety speed Cluster 1 0.86830982 0.5282993 0.8952494 0.93322588 0.5295407 1 2 -1.15981355 -0.1062743 -0.9446355 -0.91358591 -0.4716217 2 3 0.22194885 -0.9606064 0.3494969 -0.16288116 -0.5958141 3 4 -0.07448606 0.4834435 -0.3800472 0.02612117 0.4343635 4 km.cent$plot "],["principal-components-analysis.html", "Chapter 6 Principal Components Analysis 6.1 Introduction 6.2 Preparation 6.3 Base R 6.4 User Defined Function", " Chapter 6 Principal Components Analysis Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html Data for this chapter: The greekbrands.rdata is used. Load it now. # You may need to change the directory load(&quot;Data/greekbrands.rdata&quot;) 6.1 Introduction Base R is typically sufficient for performing the basics of principal components analysis, but to get some of the outputs required more easily and more efficiently, I have created a user defined function. You should download this function and save it in your working directory. pcaex.R provides eigenvalue table, scree plot, unrotated and rotated factor loading tables, and the principal components R object Once saved in your working directory, it is good practice to source user defined functions. source(&quot;pcaex.R&quot;) 6.2 Preparation For principal components analysis (PCA), we need to pass a data frame to the user defined function containing the variables to be used. Although we can pass a dataframe with additional variables, I find it easier to create a new dataframe with only those variables we want to include ( and maybe a preference and group/brand variable, see below) In our class, the data set will often be used for creating a perceptual map, so we may also have a grouping variable (e.g., brand or product name) and a preference variable Package dplyr is usually the best tool for this For this tutorial, we will perform a PCA using the greekbrands dataframe We will use only the following attributes: \\(perform\\), \\(leader\\), \\(fun\\), \\(serious\\), \\(bargain\\), \\(value\\) The data also has a preference variable, \\(pref\\) and a brand variable, \\(brand\\) library(dplyr) # Store variables selected using dplyr to &#39;segvar&#39; pcadata &lt;- greekbrands %&gt;% select(perform, leader, fun, serious, bargain, value, pref, brand) 6.3 Base R 6.3.1 prcomp() function The prcomp() function performs PCA * Usage: prcomp(formula, data=, scale=TRUE, rank=) where: * formula is a formula with no response variables, but rather only the numeric variables to be included if not all variables in data are to be included * No response variable means the formula is written as: ~var1 + var2 + var3 + ... + var4 * data= is the name of the dataframe * scale=TRUE standardizes the variables before running the PCA * rank= is the number of components to retain; default is all * When saved to an object, the following components are saved: * $sdev is the standard deviations of the principal components (i.e., the square roots of the eigenvalues) * $rotation is the unroated factor loadings * $x is the factor scores * Example: perform PCA on \\(serious\\), \\(fun\\), \\(bargain\\), \\(trendy\\), and \\(value\\) with only two components retained ```r pcaout &lt;- prcomp(~serious+fun+bargain+trendy+value, # Variables to include data=greekbrands, # Data frame to use scale=TRUE, # Standardize the variables rank=2) # Retain only first two components ``` 6.3.2 Eigenvalue table To get the eigenvalue table, we must square the $sdev component of the PCA object eigtable &lt;- data.frame(components=seq(1:5), # column with component #&#39;s pcaout$sdev^2) # Eigenvalues eigtable components pcaout.sdev.2 1 1 2.0722697 2 2 1.2747721 3 3 0.8111322 4 4 0.5913799 5 5 0.2504461 6.3.3 Unrotated loadings Unrotated factor loadings are in the $rotation component of the PCA object pcaout$rotation PC1 PC2 serious 0.006484917 -0.73514845 fun -0.179503188 0.65827780 bargain 0.596338877 0.07600771 trendy -0.469318304 -0.14191980 value 0.625984684 0.01756980 Rotated loadings Rotated loadings are not automatically created. Instead, we must use the varimax(pcaobject$rotation)$loadings command to obtain them. By default, only rotated loadings greater than 0.4 are shown. varimax(pcaout$rotation)$loadings Loadings: PC1 PC2 serious -0.101 -0.728 fun 0.677 bargain 0.601 trendy -0.485 value 0.622 PC1 PC2 SS loadings 1.0 1.0 Proportion Var 0.2 0.2 Cumulative Var 0.2 0.4 6.4 User Defined Function The pcaex.R user defined function can produce the results with one or two passes of the function Requires the following packages: ggplot2 dplyr The results should be saved to an object Usage: pcaex(data, group=\"\", pref=\"\", comp=) where: data is the PCA variable data group=\"\" is the name of the grouping variable Can be excluded if no grouping variable pref=\"\" is the name of the preference variable Can be excluded if no preference variable comp= is the number of components to retain Default is NULL if all components are wanted Objects returned: If comp is NOT provided: Scree plot ($plot) Table of eigenvalues ($table) If comp is provided: Table of eigenvalues ($table) Unrotated factor loading table ($unrotated) Rotated factor loading table ($rotated) PCA object ($pcaobj) When a group= variable is provided, the PCA will be performed on an aggregated data frame (i.e., mean values by group) Examples: All components # pcaex.R need to be &quot;sourced&quot; gb.all &lt;- pcaex(pcadata, # PCA data created earlier group=&quot;brand&quot;, # Grouping variable pref=&quot;pref&quot;) # Preference variable # Do not include &#39;comp&#39; to get all components # Call gb.all$table to get eigenvalue table gb.all$table Component Eigenvalue Difference Proporation Cumulative 1 1 3.4180 1.5468 0.5697 0.5697 2 2 1.8712 1.4213 0.3119 0.8815 3 3 0.4500 0.2527 0.0750 0.9565 4 4 0.1973 0.1567 0.0329 0.9894 5 5 0.0405 0.0175 0.0068 0.9962 6 6 0.0231 NA 0.0038 1.0000 # Call gb.all$plot to get scree plot gb.all$plot Two components gb.2comp &lt;- pcaex(pcadata, # PCA data created earlier group=&quot;brand&quot;, # Grouping variable pref=&quot;pref&quot;, # Preference variable comp=2) # Request 2 components # Call gb.2comp$unrotated to get unrotated factor loadings gb.2comp$unrotated PC1 PC2 Unexplained perform 0.4455 0.0934 0.3054 leader 0.4970 0.1817 0.0941 fun -0.5005 -0.0351 0.1416 serious 0.4712 0.2503 0.1239 bargain 0.1718 -0.6815 0.0299 value 0.2293 -0.6557 0.0159 # Call gb.2comp$rotated to get rotated factor loadings gb.2comp$rotated PC1 PC2 Unexplained perform 0.4533 -0.0409 0.3054 leader 0.5284 0.0284 0.0941 fun -0.4889 0.1127 0.1416 serious 0.5238 0.1016 0.1239 bargain -0.0350 -0.7020 0.0299 value 0.0275 -0.6941 0.0159 "],["pca-perceptual-maps.html", "Chapter 7 PCA Perceptual Maps 7.1 Introduction 7.2 Base R 7.3 User Defined Function", " Chapter 7 PCA Perceptual Maps Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html Data for this chapter: The greekbrands.rdata is used. Load it now. # You may need to change the directory load(&quot;Data/greekbrands.rdata&quot;) 7.1 Introduction Base R can produce basic perceptual maps from a PCA object, but cannot create a joint-space map easily. For joint-space maps, better looking perceptual maps, and to make the process easier, user defined function has been created percmap.R returns a ggplot created percpetual (or joint-space) map. Once saved in your working directory, it is good practice to source user defined functions. source(&quot;percmap.R&quot;) 7.2 Base R 7.2.1 Preparation For PCA perceptual maps with a grouping variable (e.g., brand or product name), it is best to use a dataframe of aggregated means by the grouping variable. For this tutorial, we will create a PCA perceptual map using the greekbrands dataframe We will use only the following attributes: \\(perform\\), \\(leader\\), \\(fun\\), \\(serious\\), \\(bargain\\), \\(value\\) The grouping variable is \\(brand\\) library(dplyr) # Store variables selected using dplyr to &#39;pmdata&#39; pmdata &lt;- greekbrands %&gt;% select(perform, leader, fun, serious, bargain, value, brand) %&gt;% group_by(brand) %&gt;% summarise_all(mean) 7.2.2 Perform PCA first To create the perceptual map, we must first run and store a PCA object on our data frame pcaout &lt;- prcomp(~perform+leader+fun+serious+bargain+value, data=pmdata, scale=TRUE, rank=2) 7.2.3 Create map using biplot The map is created using the biplot function with the PCA object passed to it along with group variable labels biplot(pcaout, # PCA object from above xlabs=pmdata$brand) # Grouping variable labels 7.3 User Defined Function 7.3.1 Preparation For the user defined function, we must pass the function a data frame with the variables to be used in creating the principal components, the grouping variable (i.e., brand/product name), and if applicable, the preference variable The data frame does not need to be aggregated, but must contain ONLY those variables library(dplyr) # For a perceptual map, store variables selected using dplyr to &#39;pmdata&#39; pmdata &lt;- greekbrands %&gt;% select(perform, leader, fun, serious, bargain, value, brand) # For a joint space map, store variable selected using dplyr to &#39;jsmdata&#39; jsmdata &lt;- greekbrands %&gt;% select(perform, leader, fun, serious, bargain, value, pref, brand) 7.3.2 Using the function The percmap.R user defined function can easily product a perceptual map or joint-space map based on the first two principal components Requires the following packages: ggplot2 dplyr Usage: percmap(data, group=\"\", pref=\"\") where: data is the PCA variable data group=\"\" is the name of the grouping variable pref=\"\" is the name of the preference variable Can be excluded if no preference variable Returns: A perceptual map if pref is not included A joint space map if pref is included Examples: Perceptual Map source(&quot;percmap.R&quot;) percmap(pmdata, group=&quot;brand&quot;) Joint Space Map percmap(jsmdata, group=&quot;brand&quot;, pref=&quot;pref&quot;) "],["standard-mnl.html", "Chapter 8 Standard MNL 8.1 Introduction 8.2 Data Preparation 8.3 Standard MNL using multinom()", " Chapter 8 Standard MNL Sources for this chapter: R for Marketing Research ad Analytics, Second Edition (2019). Chris Chapman and Elea McDonnell Feit http://r-marketing.r-forge.r-project.org/index.html Data for this chapter: The bfast.rdata is used. Load it now. # You may need to change the directory load(&quot;Data/bfast.rdata&quot;) 8.1 Introduction Base R is not good for standard multinomial logistic regression (MNL). The best package that I have found for standard MNL is nnet with its multinom function. Install that package install.packages(\"nnet\") and load it. library(nnet) In addition, I have created some user defined functions. You should download these functions and save them in your working directory. stmnl.R produces Odds Ratio coefficients table, overall model significance, and McFaddens Pseudo-\\(R^2\\) stmnl_cm.R produces a Classification Matrix stmnl_pp.R produces average predicted probability tables and plots 8.2 Data Preparation As with binary logistic regression, we often us a training and holdout sample when using MNL. For standard MNL, the process is the same. library(caret) set.seed(4320) inTrain &lt;- createDataPartition(y=bfast$bfast, p=.75, list=FALSE) train &lt;- bfast[inTrain,] test &lt;- bfast[-inTrain,] 8.3 Standard MNL using multinom() Standard MNL is performed using the multinom() function from the nnet package Usage: multinom(formula, data) formula is represented by dependent variables on the left side separated from the independent variables on the right side by a tilde(~), such as: dv ~ iv1 + iv2 data is the name of the (usually) training data As with other analyses, we save the result of the model to an object summary() provides standard coefficient estimates (i.e., not Odds Ratio estiamtes), but does not provide overall model fit values (i.e., overall model \\(p\\)-value or McFaddens Psuedo \\(R^2\\)) or p-values for each independent variable library(nnet) model &lt;- multinom(bfast ~ gender + marital + lifestyle + age, data=train) # weights: 18 (10 variable) initial value 727.281335 iter 10 value 579.014122 final value 574.997631 converged summary(model) Call: multinom(formula = bfast ~ gender + marital + lifestyle + age, data = train) Coefficients: (Intercept) genderMale maritalUnmarried lifestyleInactive age Bar 0.8832457 -0.21298963 0.6126977 -0.7865772 -0.02532866 Oatmeal -4.4920408 -0.02262325 -0.3897362 0.3187473 0.07996475 Std. Errors: (Intercept) genderMale maritalUnmarried lifestyleInactive age Bar 0.3256994 0.2064320 0.2123832 0.2090460 0.006655803 Oatmeal 0.4596750 0.2094666 0.2366511 0.2156992 0.007755708 Residual Deviance: 1149.995 AIC: 1169.995 8.3.1 stmnl() User Defined Function To get overall model fit values, as well as an Odds Ratio estimate table with p-values, the stmnl() user defined function can be used Usage: stmnl(model) model is the name of the object with the saved model results NOTE: This function requires the broom package. If you do not have it installed on your machine, you will need to install it. source(&quot;stmnl.R&quot;) stmnl(model) Loading required package: broom Registered S3 methods overwritten by &#39;broom&#39;: method from tidy.glht jtools tidy.summary.glht jtools LR chi2 (8) = 288.1568; p &lt; 0.0001 McFadden&#39;s Pseudo R-square = 0.2004 y.level term estimate std.error statistic p.value Bar (Intercept) 2.4187 0.3257 2.7118 0.0067 Bar genderMale 0.8082 0.2064 -1.0318 0.3022 Bar maritalUnmarried 1.8454 0.2124 2.8849 0.0039 Bar lifestyleInactive 0.4554 0.2090 -3.7627 0.0002 Bar age 0.9750 0.0067 -3.8055 0.0001 Oatmeal (Intercept) 0.0112 0.4597 -9.7722 0.0000 Oatmeal genderMale 0.9776 0.2095 -0.1080 0.9140 Oatmeal maritalUnmarried 0.6772 0.2367 -1.6469 0.0996 Oatmeal lifestyleInactive 1.3754 0.2157 1.4777 0.1395 Oatmeal age 1.0832 0.0078 10.3104 0.0000 8.3.2 Classification Matrix To get a classification matrix, the stmnl_cm user-defined function should be used Usage: stmnl_cm(model, data) model is the name of the object with the saved model results data is name of the data to create the classification model for (i.e., training or holdout data) source(&quot;stmnl_cm.R&quot;) stmnl_cm(model, train) 0.5619 = Hit Ratio 0.3413 = PCC Level T.Cereal T.Bar T.Oatmeal Total 1 P.Cereal 124 85 46 255 2 P.Bar 52 68 7 127 3 P.Oatmeal 79 21 180 280 4 Total 255 174 233 662 stmnl_cm(model, test) 0.5826 = Hit Ratio 0.3416 = PCC Level T.Cereal T.Bar T.Oatmeal Total 1 P.Cereal 45 24 20 89 2 P.Bar 18 25 0 43 3 P.Oatmeal 21 8 57 86 4 Total 84 57 77 218 8.3.3 Average Predicted Probabilities Predicted probabilities can help interpret the effects of the independent variables on the choice dependent variable Use the stmnl_pp user-defined function for each IV to obtain these. Usage: stmnl_pp(model, focal, xlab) model is the name of the object with the saved model results focal is the name of the variable (in quotes) for which predicted probabilities are wanted xlab is optional, but can be provided for a better x-axis label for the plot (e.g., Income Category) NOTE 1: For continuous focal variables, the table produced uses three levels to calculate predicted probabilities: \\(-1 SD\\), \\(MEAN\\), \\(+1 SD\\) NOTE 2: This function requires the following packages: tidyr effects dplyr ggplot2 source(&quot;stmnl_pp.R&quot;) stmnl_pp(model, &quot;age&quot;, &quot;Age in Years&quot;) $table age bfast p.prob lower.CI upper.CI 1 31 Cereal 0.5158 0.5727 0.4586 2 31 Bar 0.4134 0.4718 0.3573 3 31 Oatmeal 0.0708 0.1047 0.0473 4 49 Cereal 0.4792 0.5274 0.4314 5 49 Bar 0.2434 0.2874 0.2043 6 49 Oatmeal 0.2773 0.3255 0.2338 7 67 Cereal 0.2657 0.3196 0.2180 8 67 Bar 0.0856 0.1199 0.0604 9 67 Oatmeal 0.6487 0.7035 0.5897 $plot stmnl_pp(model, &quot;gender&quot;, &quot;Gender&quot;) $table gender bfast p.prob lower.CI upper.CI 1 Female Cereal 0.4660 0.5276 0.4053 2 Female Bar 0.2634 0.3219 0.2122 3 Female Oatmeal 0.2706 0.3324 0.2166 4 Male Cereal 0.4939 0.5584 0.4296 5 Male Bar 0.2257 0.2848 0.1758 6 Male Oatmeal 0.2804 0.3473 0.2221 $plot "],["alt-specific-mnl.html", "Chapter 9 Alt-Specific MNL 9.1 Introduction 9.2 Alt-Spec MNL using User Defined Function", " Chapter 9 Alt-Specific MNL Data for this chapter: The yogurt.rdata is used. Load it now. Because of how the data is arranged, the training and holdout samples are both included in the file: train.yog and test.yog. # You may need to change the directory load(&quot;Data/yogurt.rdata&quot;) 9.1 Introduction Base R is not good for alternative specific multinomial logistic regression (MNL). The best package that I have found for alternative specific MNL is mlogit with its mlogit function. Use install.packages(\"mlogit\") to install the package on your machine, then load it using the library function when needed. library(mlogit) Loading required package: dfidx Attaching package: &#39;dfidx&#39; The following object is masked from &#39;package:stats&#39;: filter However, even that is not very user friendly (in my opinion). Therefore, I have written a few user defined functions to help with getting the necessary results from an alternative specific MNLInstall that package install.packages(\"nnet\") and load it. * asmnl_est.R produces Odds Ratio coefficients table, overall model significance, McFaddens Pseudo-\\(R^2\\), and classification matrices for both the training data and the test/holdout data * asmnl_me.R produces marginal effects tables for all IVs * asmnl_mp.R produces margin plots for case-specific IVs 9.2 Alt-Spec MNL using User Defined Function Alternative Specific MNL is performed using the asmnl_est.R user defined function Usage: asmnl_est(formula, data, id=\"\", alt=\"\", choice=\"\", testdata) formula is an object with a saved formula. The formula is represented by a DV on the left side separated from the IVs on the right side by a tilde(~). The IVs are further separated by having choice-specific variables on the left and the case-specific variables on the right, separated by a vertical line, |. For example: myform = choice ~ chvar1 + chvar2 | casvar1 + casvar2 data is the name of the training data id is the variable that identifies the case (in quotes) alt is the variable that identifies the choice (in quotes) choice is the variable that identifies if alt was selected or not testdata is the name of the test data The function will display the coefficients table, overall model significance, McFaddens Pseudo-\\(R^2\\), and classification matrices for both the training data and the test/holdout data In addition, the results should be saved to an object to be used in other user defined functions NOTE 1: To work properly, all factor IVs should already be in dummy variable coding NOTE 2: This function also requires the broom package library(mlogit) # Saving formula to object myform &lt;- choice ~ feat + price | income source(&quot;asmnl_est.R&quot;) asmod &lt;- asmnl_est(formula=myform, data=train.yog, id=&quot;id&quot;, alt=&quot;brand&quot;, choice=&quot;choice&quot;, testdata=test.yog) --------- Model Fit --------- Log-Likelihood: -1618.4208 McFadden R^2: 0.2397 Likelihood ratio test: chisq = 1020.5649 (p.value &lt; .0001) --------------------- OR Estimation Results --------------------- term estimate std.error statistic p.value (Intercept):Hiland 2.1355 0.5677 1.3365 0.1814 (Intercept):Weight 0.9740 0.2079 -0.1269 0.8990 (Intercept):Yoplait 0.0185 0.2680 -14.8845 0.0000 feat 1.5267 0.1491 2.8371 0.0046 price 0.6425 0.0296 -14.9691 0.0000 income:Hiland 0.8975 0.0149 -7.2464 0.0000 income:Weight 0.9886 0.0038 -3.0436 0.0023 income:Yoplait 1.0756 0.0040 18.1030 0.0000 --------------------------------------- Classification Matrix for Training Data --------------------------------------- 0.6207 = Hit Ratio 0.3299 = PCC T.Dannon T.Hiland T.Weight T.Yoplait Total P.Dannon 577 39 324 97 1037 P.Hiland 1 12 0 2 15 P.Weight 18 2 38 18 76 P.Yoplait 132 1 53 497 683 Total 728 54 415 614 1811 -------------------------------------- Classification Matrix for Holdout Data -------------------------------------- 0.6073 = Hit Ratio 0.3309 = PCC T.Dannon T.Hiland T.Weight T.Yoplait Total P.Dannon 199 14 104 38 355 P.Hiland 2 2 1 1 6 P.Weight 8 1 12 13 34 P.Yoplait 33 0 21 152 206 Total 242 17 138 204 601 9.2.1 Marginal Effects The asmnl_me.R user defined function will be used to get the marginal effects of the IVs Usage: asmnl_me(mod) mod is the object containing the result of the mlogit call using the asmnl_est user defined function source(&quot;asmnl_me.R&quot;) asmnl_me(asmod) -------------------------------- Predicted Probabilities at Means -------------------------------- Dannon Hiland Weight Yoplait 0.4832 0.0048 0.2571 0.2549 ------------------------- Marginal effects for feat ------------------------- Dannon Hiland Weight Yoplait Dannon 0.10565 -0.00098 -0.05256 -0.05212 Hiland -0.00098 0.00201 -0.00052 -0.00052 Weight -0.05256 -0.00052 0.08080 -0.02773 Yoplait -0.05212 -0.00052 -0.02773 0.08036 -------------------------- Marginal effects for price -------------------------- Dannon Hiland Weight Yoplait Dannon -0.11049 0.00102 0.05496 0.05450 Hiland 0.00102 -0.00211 0.00054 0.00054 Weight 0.05496 0.00054 -0.08450 0.02899 Yoplait 0.05450 0.00054 0.02899 -0.08404 --------------------------- Marginal effects for income --------------------------- Dannon Hiland Weight Yoplait -0.00731 -0.00059 -0.00684 0.01473 9.2.2 Margin Plots The asmnl_mp.R user defined function will create margin plots for a case-specific IV Usage: almnl_mp(mod, focal=\"\", type=\"\") *modis the object containing the result of themlogitcall using theasmnl_estuser defined function *focalis the case-specific IV for which a margin plot is wanted (in quotes) *typeis the type of IV; must be eitherCfor continuous orD` for dummy NOTE: This function requires the ggplot2 package source(&quot;asmnl_mp.R&quot;) asmnl_mp(asmod,&quot;income&quot;, &quot;C&quot;) "],["conjoint-analysis.html", "Chapter 10 Conjoint Analysis 10.1 Introduction 10.2 tradca User Defined Function 10.3 cademo User Defined Function 10.4 capred User Defined Function", " Chapter 10 Conjoint Analysis Data for this chapter: The airlineca.rdata is used. Load it now. Because of how the data is arranged, the training and holdout samples are both included in the file: train.yog and test.yog. # You may need to change the directory load(&quot;Data/airlineca.rdata&quot;) 10.1 Introduction Base R does not have any built in function to handle conjoint analysis, because conjoint analysis can be done in a variety of ways. For this tutorial, the focus is on traditional conjoint using linear regression based methods. To accomplish the conjoint analysis, I have written a three user defined functions to help with getting the necessary results for traditional conjoint. tradca.R produces overal part-worth plots for each attribute, and importance plot, and a table with case-level part-worths and importances cademo.R produces correlation matrices and importance regressions from a dataframe of case-level part-worths/importances that has been appended with demographics capred.R compares two profiles specified by the user 10.2 tradca User Defined Function Usage: tradca(formula, data, idvar=\"\") formula is an object with a saved formula. The formula is represented by a DV on the left side separated from the IVs on the right side by a tilde(~). For traditional conjoint, the DV is the value rating for each profile, while the IVs are the name of the attribute variables. For example: caform = value ~ attr1 + attr2 + attr3 data is the name of the dataframe with the value ratings for each profile idvar is the variable that identifies the case (in quotes) When saved to an object, this function will return three things: pwplot contains a plot with average part-worth values for each attribute impplot contains a plot with the average importances casetable contains a dataframe with the part-worths and importances for each case NOTE: This function requires the following packages: broom dplyr stringr ggplot2 # Load necessary packages library(broom) library(dplyr) library(stringr) library(ggplot2) # Saving formula to object caform &lt;- value ~ airline + connect + price # Loading user defined function source(&quot;tradca.R&quot;) # Using fucntion and saving to object called &#39;results&#39; results &lt;- tradca(formula = caform, # Formula object created above data = airlineca, # Data frame with profile ratings idvar=&quot;caseid&quot;) # Case id variable Warning: `funs()` was deprecated in dplyr 0.8.0. Please use a list of either functions or lambdas: # Simple named list: list(mean = mean, median = median) # Auto named with `tibble::lst()`: tibble::lst(mean, median) # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) This warning is displayed once every 8 hours. Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. results$pwplot # Get part worth plot results$impplot # Get importance plot 10.3 cademo User Defined Function Usage: cademo(formula, data, vars=\"\") formula is the saved formula object from before data is the name of the dataframe with case-level part-worths and importances, along with the appended demographic variables (see Preparation below) vars is a list of the demographic variable names When run, this function will return correlation matrices and importance regressions depending on the type of demographic variable NOTE: This function requires the Hmisc package 10.3.1 Preparation To use this function, the casetable from the call to the tradca function must be appended with the demographic variables for each case. To do this, use the following code: demos &lt;- airlineca %&gt;% # Create new dataframe called &#39;demos&#39; based on original data group_by(caseid) %&gt;% # Group by the &#39;idvar&#39; summarise(inc=first(inc), # Capture the demographic variables type=first(type)) %&gt;% bind_cols(.,results$casetable[,2:ncol(results$casetable)]) # NOTE: This last command appends the demographic variables &#39;casetable&#39; dataframe 10.3.2 Function usage source(&quot;cademo.R&quot;) # Load the function&#39; cademo(caform, # Formula object from before demos, # Dataframe created above c(&quot;inc&quot;, &quot;type&quot;)) # Names of the demographic variables Correlation Matrix for airline_ Loading required package: Hmisc Loading required package: survival Attaching package: &#39;survival&#39; The following object is masked from &#39;package:caret&#39;: cluster Loading required package: Formula Attaching package: &#39;Hmisc&#39; The following object is masked from &#39;package:jtools&#39;: %nin% The following objects are masked from &#39;package:dplyr&#39;: src, summarize The following objects are masked from &#39;package:base&#39;: format.pval, units inc airline_1: Delta 0.36* airline_2: Spirit 0.17 airline_3: SW 0.36* Correlation Matrix for connect_ inc connect_1: None -0.01 connect_2: One 0.20 Correlation Matrix for price_ inc price_1: $300 -0.35* price_2: $450 -0.17 price_3: $600 -0.09 Correlation Matrix for Imp inc Imp_airline 0.47* Imp_connect 0.17 Imp_price -0.48* Regression Results for Imp_airline Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 10.1131 7.5592 1.3379 0.1891 inc 0.3325 0.0946 3.5161 0.0012 typePleasure -9.4936 5.3744 -1.7664 0.0856 Regression Results for Imp_connect Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 10.2144 5.3877 1.8959 0.0658 inc 0.0787 0.0674 1.1672 0.2506 typePleasure -5.0497 3.8305 -1.3183 0.1955 Regression Results for Imp_price Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 79.6701 8.7485 9.1068 0.0000 inc -0.4111 0.1094 -3.7567 0.0006 typePleasure 14.5439 6.2200 2.3382 0.0249 10.4 capred User Defined Function Usage: cademo(formula, data, prof1, prof2) formula is the saved formula object from before data is the name of the dataframe with case-level part-worths and importances; this is the casetable object from the original call to the tradca function prof1 is the first profile (see Preparation below) prof2 is the second profile (see Preparation below) When run, this function will return correlation matrices and importance regressions depending on the type of demographic variable 10.4.1 Preparation To use this function, it works best to create the profiles by saving them to separate objects. The attribute levels for the profiles should be in the form of: attribute_levelnumber. For example, for the first Delta is the first level of the airline attribute, so we would use airline_1 to indicate that level of that attribute # Create first profile prof1 &lt;- c(&quot;airline_1&quot;, &quot;connect_2&quot;, &quot;price_3&quot;) # Create second profile prof2 &lt;- c(&quot;airline_3&quot;, &quot;connect_2&quot;, &quot;price_2&quot;) 10.4.2 Function usage source(&quot;capred.R&quot;) # Load the function&#39; capred(caform, # Formula object from before results$casetable, # Dataframe from original &#39;tradca&#39; call prof1, prof2) # Names of our profile objects Profile 1 = airline_1: Delta / connect_2: One / price_3: $600 Mean Utility = 26.382 95% CI = (18.107,34.657) Profile 2 = airline_3: SW / connect_2: One / price_2: $450 Mean Utility = 51.933 95% CI = (42.208,61.658) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
